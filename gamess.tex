\section{Real-World Application Case Study: GAMESS}
\label{section:applications}

In order to further test \chipstar in practice, we ported a %set of 
complex HIP/CUDA-based HPC application and its dependency library using \chipstar. The test environment for the experiment was the Aurora supercomputer utilizing Intel Datacenter Intel® Data Center GPU Max Series (referred to from here on as PVCs, as in Ponte Vecchio) as the accelerator part~\cite{aurora}.
%The porting examples are described in the following subsections with the performance evaluations
%presented in the next section.
%The applications and which features or libraries of \chipstar they use are shown in Table~\ref{tab:applications}.

\subsection{GPU Integral Library (GAMESS-EXESS)}

General Atomic and Molecular Electronic Structure System (GAMESS~\cite{gamess,gamess2}) is a quantum chemistry software package which implements many electronic structure methods. 
The code base is primarily in Fortran 77/90 with some C/C++ and a CUDA library. Recently a new GPU version of the Hartree-Fock (HF) and RI-MP2 methods were implemented in CUDA which scales to 4096 nodes on Summit, an Nvidia V100-based supercomputer \cite{gamess_cuda1, gamess_cuda2, summit}.
In this porting case we focused on the Hartee-Fock (HF) algorithm used by a CUDA library in GAMESS described in \cite{gamess_cuda1}, which has been ported to HIP. The HF method is a common quantum chemistry method which is often the starting point for other higher-accuracy methods.  The HF method determines the molecular energy of a system by solving a set of non-linear eigenvalue equations iteratively.  It primarily involves the computation of $N^4$ two electron integrals (where $N$ is a measure of molecular system size) as well as matrix contractions of the two electron integrals once they are formed.
% Colleen> I'm ok to remove the discussion of the basis functions if the code works for all of them :) The two electron integrals are grouped into different classes, depending on the angular momentum of the basis functions used. The basis functions here are $s-$ ,$p-$, and $d-$, where $s$ is least complex and $d$ is the most complex.

The two electron integrals are implemented as HIP/CUDA kernels which were optimized for Nvidia GPUs and total over 20,000 lines of HIP/CUDA kernel code.
%From the non-basic features of HIP supported by \chipstar, the kernels use shared memory with \func{\_\_syncthreads()} calls to ensure copying values from global memory to shared memory completed for the threadblock before using it.\pj{Colleen: does it use any other "special" CUDA/HIP features on the host side?}
%

\subsection{hipBLAS and hipSOLVER}

Since the application uses ROCm software platform libraries hipBLAS and hipSOLVER, they needed to be ported as well. The required interfaces of these libraries were implemented for Intel hardware by using oneMKL as a backend. %Applications calling hipBLAS functions can thus run on Intel hardware without any code changes. 
For this porting case, a SYCL interoperatibility feature was added to \chipstar which was used to invoke oneMKL’s SYCL functions efficiently.

\pj{TODO (Colleen/?): A brief description how hipSOLVER was implemented on MKL?}

%hipBLAS and hipSOLVER calls are used to form intermediates. The main hipBLAS calls are hipblasDscal, hipblasDgemm, hipblasDcopy, hipblasDaxpy, hipblasDdot, hipblasDgemv, hipblasDgeam, and the main hipSOLVER call is hipsolverDsyevd.

%\pj{From these it would be good to summarize for example, that they cover the key blas functionality.} These are used at each iteration of the HF algorithm to (among other things) diagonalize the Fock matrix and construct the density matrix, which are key blas functionalities in the HF algorithm.

\subsection{Porting Notes}

In terms of functionality, the HF code compiles and was verified to run correctly with \chipstar on PVCs. The porting effort was relatively low, with one exception due to a small but significant specification difference in CUDA vs. OpenCL related to kernel thread synchronization: In CUDA group barriers are not counting in exited threads, meaning that there can be early returns from the kernel by a subset of the threads after which it is still legal to perform barrier synchronization with the remaining subset -- the exited threads are just not counted in. In OpenCL this case is undefined behavior and in many implementations can lead to a deadlock. To tackle this gap, an OpenCL extension adding a group barrier with similar semantics would be needed (see \textit{cl\_ext\_alive\_only\_barrier} in Table~\ref{table:extensions}).

\subsection{Performance}

%The GAMESS port was executed with the PVC component of the Aurora installation. 
The performance of GAMESS was measured by compiling and running the same HIP source code on a PVC through \chipstar and on an Nvidia A100 as well as an AMD MI250 using ROCm 6.0.0. 
%\pj{Is it running the HIP code via AMD HIP wrapper or the CUDA version?} \cb{via the AMD HIP wrapper -- everything is using the same HIP code (I put all the runscripts and output on the github https://github.com/colleeneb/gamess\_libcchem\_hip for the record. so it was https://github.com/colleeneb/gamess\_libcchem\_hip/blob/hip\_dev\_for\_intel/hip\_nvidiaa100.sh)} 
% Not sure if this context is needed here: --PJ
%The AMD MI250 is part of the JLSE cluster at ANL and is a Supermicro AS-4124GQ-TNMI composed of 2 AMD EPYC 7713 64c (Milan) CPUs and 4 AMD Instinct MI250~\cite{JLSE}. The Nvidia A100 is also part of the JLSE cluster and is composed of a AMD 7532 and 1 Nvidia A100 with 40GB and PCIe 4.0. 
The test run computed the HF energy of a cluster of 150 water molecules with a STO-3G basis set. The results are displayed in Table~\ref{table:gamess_perf}.

\begin{table*}[ht]
\centering
\begin{tabular}{l|l|l|l|l}
                   & Nvidia A100 & AMD MI250 & Intel PVC      & Intel PVC \\
                   &               &             & (OpenCL backend) & (Level Zero backend) \\ \hline
Total SCF time (s) &    1.998      &    26.09    & 4.9  & 3.6  
\end{tabular}
    \caption{Comparison of GPU integral code performance across Intel, AMD, and Nvidia}
    \label{table:gamess_perf}
\end{table*}

Table~\ref{table:gamess_perf} shows that the execution time on the Nvidia A100 is shortest and on AMD MI250 the longest. The energy calculation of the GAMESS simulation can be split into two main parts: Fock build time (time for computation of electron repulsion integrals and Fock matrix), and DIIS time (time for solving a set of linear equations). The Fock build time is primarily hand-written HIP kernels. The DIIS time is primarily BLAS and LAPACK calls, including calls to the hipSOLVER function hipsolverDsyevd. Table~\ref{table:gamess_breakdown} shows the timing breakdown for each architecture. Compared to the A100 times, the execution times on Intel PVC with the Level Zero backend are 2-5x slower. Although the Fock build time for the Intel PVC with the OpenCL backend is only 3x slower than the A100 time, the times are 25-53x slower for the DIIS and hipsolverDsyevd times. Similarly, although the Fock build time for the MI250 time is only 1.4x slower than the A100 time, and the DIIS time without hipsolverDsyevd is 0.7x the A100 time, the hipsolverDsyevd time is 109x the A100 time.\pj{TODO: Still need to identify why CL is so much slower. But is the LZ number good or not - how is A100 vs. PVC in terms of peak perf?}

% Colleen needs to update this section

\begin{table*}[ht]
\centering
\begin{tabular}{l|l|l|l|l}
 &
  Fock time &
  DIIS time without hipsolverDsyevd &
  hipsolverDsyevd time &
  Remainder \\
 &
  (ratio over A100 time) &
  (ratio over A100 time) &
  (ratio over A100 time) &
  (ratio over A100 time) \\ \hline
Nvidia   A100       & 1.46 (1x) & 0.183 (1x) & 0.230 (1x) & 0.128 (1x) \\
AMD MI250           & 2.03 (1.4x)  & 0.12 (0.7x)  &  25.10 (109x) & 0.09 (0.7x) \\
Intel PVC   (OpenCL) & 4.29 (2.9x)  & 4.73 (25.8x) & 12.34 (53.7x) & 0.54 (4.2x) \\
Intel PVC   (LevelZero)    & 3.11 (2.1x) & 0.954 (5.2x) & 0.984 (4.3x) & 0.262 (2x)
\end{tabular}
    \caption{Timing breakdown of the GPU integral HIP code across Intel, AMD, and Nvidia}
    \label{table:gamess_breakdown}
\end{table*}
