\documentclass[Afour,sagev,times]{sagej}

% Begin IEEE Imports
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{soul}

%Removed the DRAFT stamp in the hopes of this being
%the final submission ;) --Pekka
%\usepackage{draftwatermark}
%\SetWatermarkText{DRAFT}
%\SetWatermarkScale{1}
%\SetWatermarkLightness{0.9}

% https://tex.stackexchange.com/questions/326897/vertical-alignment-of-a-turned-cell
\usepackage{rotating}
\usepackage{array,makecell,multirow}

\newboolean{showhighlights}
\setboolean{showhighlights}{false}
\ifthenelse{\boolean{showhighlights}}
{ \newcommand{\myhl}[1]{\hl{#1}} }
{ \newcommand{\myhl}[1]{#1} }


\usepackage{ifthen}
\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
{ \newcommand{\mynote}[3]{
     \fbox{\bfseries\sffamily\scriptsize#1}
        {\small$\blacktriangleright$\textsf{\emph{\color{#3}{#2}}}$\blacktriangleleft$}}
  \newcommand{\newtext}[1]{{\color{orange}{#1}}}}
{ \newcommand{\mynote}[3]{}
  \newcommand{\newtext}[1]{#1}}

\usepackage[acronym]{glossaries}
\makeglossaries

\newacronym{ir}{\myhl{IR}}{\myhl{Intermediate Representation}}
\newacronym{jit}{\myhl{JIT}}{\myhl{just-in-time}}
\newacronym{isa}{\myhl{ISA}}{\myhl{Instruction Set Architecture}}
\newacronym{hsa}{\myhl{HSA}}{\myhl{Heterogeneous System Architecture}}
\newacronym{svm}{\myhl{SVM}}{\myhl{Shared Virtual Memory}}
\newacronym{cg}{\myhl{CG}}{\myhl{Coarse-grained buffer}}
\newacronym{spir}{\myhl{SPIR}}{\myhl{Standard Portable Intermediate Representation}}
\newacronym{spirv}{\myhl{SPIR-V}}{\myhl{new version of SPIR}}
\newacronym{hsail}{\myhl{HSAIL}}{\myhl{Heterogeneous system architecture intermediate language}}
\newacronym{ssa}{\myhl{SSA}}{\myhl{Static Single Assignment}}
\newacronym{dpc}{\myhl{DPC++}}{\myhl{Data Parallel C++}}
\newacronym{usm}{\myhl{USM}}{\myhl{Unified Shared Memory}}


% Please use a named note with this macro to comment the text:
\newcommand{\pj}[1]{ \mynote{PJ}{#1}{blue} }
\newcommand{\bv}[1]{ \mynote{BV}{#1}{green} }
\newcommand{\mb}[1]{ \mynote{MB}{#1}{cyan} }
\newcommand{\cb}[1]{ \mynote{CB}{#1}{magenta} }
\newcommand{\pv}[1]{ \mynote{PV}{#1}{yellow} }
\newcommand{\ba}[1]{ \mynote{BA}{#1}{brown} }
\newcommand{\kh}[1]{ \mynote{KH}{#1}{red} }


\newcommand{\hiplz}{\texttt{HIPLZ}\xspace}
\newcommand{\hipcl}{\texttt{HIPCL}\xspace}
\newcommand{\hip}{\texttt{HIP}\xspace}
\newcommand{\opencl}{\texttt{OpenCL}\xspace}
\newcommand{\lz}{\texttt{L0}\xspace}
\newcommand{\sycl}{\texttt{SYCL}\xspace}
\newcommand{\cuda}{\texttt{CUDA}\xspace}
\newcommand{\chipstar}{\textit{chipStar}\xspace}
\newcommand{\func}[1]{$#1$\xspace}
\newcommand{\type}[1]{$#1$\xspace}

% End of IEE imports

\usepackage{moreverb,url}

\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}

\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\volumeyear{2020}
\setcounter{secnumdepth}{3} 
\begin{document}

%\runninghead{TODO}

\title{\chipstar: Making HIP/CUDA Applications Cross-Vendor Portable by Building on Open Standards}

\author{
{Paulius Velesko}\affilnum{1},
{Pekka Jääskeläinen}\affilnum{4},
{Henry Linjamäki}\affilnum{4},
{Michal Babej}\affilnum{4},
{Peng Tu}\affilnum{3},
{Sarbojit Sarkar}\affilnum{3},
{Ben Ashbaugh}\affilnum{3},
{Colleen Bertoni}\affilnum{2},
{Jenny Chen}\affilnum{9},
{Philip C. Roth}\affilnum{5},
{Wael Elwasif}\affilnum{5},
{Rahulkumar Gayatri}\affilnum{6},
{Jisheng Zhao}\affilnum{7},
{Karol Herbst}\affilnum{8},
{Kevin Harms}\affilnum{2},
{Brice Videau}\affilnum{2}}

\affiliation{
\affilnum{1}PGLC Consulting
\affilnum{2}Argonne National Laboratory
\affilnum{3}Intel Corporation
\affilnum{4}Tampere University
\affilnum{5}Oak Ridge National Laboratory
\affilnum{6}National Energy Research Scientific Computing Center
\affilnum{7}Georgia Institute of Technology
\affilnum{8}Red Hat, Inc.
\affilnum{9}Purdue University
}

\corrauth{Paulius Velesko, pvelesko@pglc.io}

% Jenny requests we use jjenny0503@gmail.com for her email


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
We describe \chipstar, an open source software stack that enables building unmodified CUDA and HIP programs into binaries that rely solely on open cross-vendor compute standards OpenCL and SPIR-V. The relevant technical aspects of \chipstar and the feature mismatches between the CUDA/HIP APIs and OpenCL are discussed along with a set of standard extension proposals to bridge the essential gaps in the future.
The key benefit of the software stack is its portability, which is demonstrated by providing performance evaluations on a diversity of less common CPU/GPU platforms including RISC-V/PowerVR and ARM Mali. A comparison against the original AMD HIP platform provides a geometric mean of 0.75, a reasonable price to pay for the enhanced portability.    
\chipstar is now considered mature enough for wider testing and even production use, which is demonstrated by successful porting and competitive performance of GAMESS-GPU-HF, a complex HPC application.

\end{abstract}

%\begin{IEEEkeywords}
\keywords{CUDA, HIP, OpenCL, SPIR-V, Portability, Shared Virtual Memory}
%\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle

%\section{Introduction}

\noindent The walled garden strategy is popular among market-dominating companies. The idea behind walled garden strategy is to lock in customers to a company's products by making escaping the gates of the garden as costly as possible. NVIDIA's CUDA software platform is considered to be one of such walled gardens. It helps NVIDIA to expand and retain their GPU market advantage, and at the same time maintain a high innovation pace on the software APIs since there is no need to work with standardization committees that always have to aim for a consensus among multiple participating vendors.

Naturally, for end-users and the competing hardware vendors, the situation of a single-vendor dictated API is not ideal. End-users would prefer open standard software interfaces that enable switching the targeted hardware without incurring significant non-recurring engineering costs required for porting the applications and libraries to a new software platform just to be able to utilize the newly purchased hardware optimally. Similarly, other hardware vendors, aiming to get their piece of the market pie, would prefer an API that is not controlled by a single vendor.

AMD's ROCm~\cite{ROCm} software platform and its Heterogeneous-compute Interface for Portability (HIP) language~\cite{hip} helps escaping the CUDA walled garden by providing a route out from the NVIDIA CUDA platform to AMD's devices. HIP defines a subset of CUDA that is more easily portable to various hardware, thanks mainly to omitting various advanced features available in the later CUDA versions.

In order to enable an easy automated transition path from CUDA applications, HIP is largely a copy of a CUDA C/C++ API subset with a few minor differences and renamed functions. HIP alleviates the CUDA portability problem, but doesn't solve it satisfactorily due to AMD targeting their self-specified low-level ROCm APIs which are not actively supported on non-AMD platforms.
An open source HIP/CUDA software platform solely based on open standards with a sincere aim for cross-vendor portability is still lacking.

With the \chipstar software stack described in this article we aim to alleviate the CUDA/HIP application
portability problem. In contrast to previous solutions that either require source-to-source conversion from CUDA programs~\cite{SYCLomatic}, that can lead to costly multiple codebase maintenance, or aim for binary-level compatibility of existing CUDA/HIP programs~\cite{ZLUDA} that rely on questionable reverse engineering of proprietary binary interfaces -- a brittle longer-term strategy -- \chipstar chooses a middle-ground approach which enables source-level compatibility of HIP/CUDA programs by compiling them to a runtime portable ``fat binary'' (a single executable that contains multiple code objects, each compiled for a different architecture)  that utilizes solely open standards and can execute on any platform supporting the required standard features without recompilation.

With this article and the associated open source code base we make the following contributions:

\begin{enumerate}
  \item We publish internal design choices for the software platform \chipstar that enables porting applications from the NVIDIA-driven CUDA and AMD-driven ROCm platforms to any current and future platform supporting the OpenCL and \acrshort{spirv} cross-vendor open standards.
  \item We evaluated \chipstar performance in comparison to running the CUDA applications directly using the NVIDIA SDK or converting the applications to a popular open-standard-based CUDA alternative SYCL.
  %~\cite{SYCL}
  \item We demonstrate a case study for the usability of OpenCL as a portability layer to implement other languages/APIs on top. Portability is shown by providing performance numbers on a RISC-V CPU \& PowerVR GPU,  ARM CPU and GPU, as well as on discrete GPUs from NVIDIA, AMD, and Intel. 
\end{enumerate}

The rest of the article is structured as follows: Section~\ref{section:portabilityAPIs} discusses our rationale for choosing OpenCL~\cite{OpenCL} and its device-side program representation SPIR-V~\cite{SPIRV} as the core APIs to support runtime portability in \chipstar.
Section~\ref{section:implementation} details the key technical issues in implementing the HIP/CUDA runtime on these APIs, while Section~\ref{section:compilation} focuses on the compilation aspects. Performance evaluation results are shown in Section~\ref{section:performance}. 
Section~\ref{section:applications} presents the GAMESS-GPU-HF porting case study using \chipstar, and finally Section~\ref{section:conclusions} concludes the article.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\texorpdfstring{\myhl{Rationale for the Chosen Standard APIs}}a}
\label{section:portabilityAPIs}

%As a technical background, we provide our considerations for the ``hardware abstraction layer'' API options forming the platform portability layer for \chipstar.
\myhl{As a technical background, we review the various options available for the portable runtime API and the target-independent device program representation in }\chipstar \myhl{and provide a rationale for our choice}. The discussion is split into 1) runtime APIs used to control the execution from the host side and 2) device program representations providing an abstraction for the kernel side programs in a portable manner. Due to the abundance of potential target devices available for acceleration, we consider it important to be able to embed the device programs in an open standard-based \gls{ir} and to use \gls{jit} compilation for lowering the program to the target \gls{isa} of the accelerator at deployment or launch time. This enables future-proof ``fat binaries'' which can be supported on new platforms by implementing the specification of the \gls{ir}. Another alternative would be to provide only source-level
compatibility where the application needs to be recompiled for each host and a device pair of interest, hindering binary distribution.

\subsection{Runtime APIs}

In practice, both CUDA and HIP are single-vendor supported programming models.
This is reflected, for example, in their platform property APIs which define limited
queries for device properties, highlighting features in each vendor's
GPU offerings. The goal of \chipstar is to
expand the portability of applications implemented using the CUDA/HIP APIs.
Therefore, the key requirement to the underlying ``platform/device portability API'' is to
cover as many of the essential features of CUDA/HIP as possible to
provide functional correctness and to exploit the potential performance benefits.
This includes,
for example, parallel and asynchronous execution of tasks, overlapping of
data transfers with task execution, and by providing access to
shared memory communication, if available. Furthermore, the portability API
should provide services to enhance performance portability of
the implementation by allowing to query the capabilities of the
devices to tune the execution at runtime to match the target's features.

There are not a large number of choices for such a runtime API, especially
if limiting the list to alternatives that enjoy official driver support from
multiple accelerator vendors or to those that have a portable long-maintained open source implementation. In this regard, an open standard based API that has increased in popularity is SYCL~\cite{SYCL}. SYCL
resembles CUDA in being a C++-based single-source API. However, as a layer for supporting other languages on top, it lacks means to explicitly load and launch pre-built kernels defined in non-SYCL language at host runtime. It has an interoperability layer that allows one to build OpenCL C kernels, but then the additional value of using SYCL as a portability layer instead of using OpenCL directly is not obvious.

Recently, OpenMP~\cite{OpenMP} has been considered for portability layer usage and,
in fact, specifically for implementing CUDA in~\cite{10.1145/3559009.3569687}.
While OpenMP enjoys support from a wide range of vendors,
we believe OpenMP is not ideal for this use case since it doesn't
define a device-side program representation, making future-proof cross-vendor portable fat binary generation difficult. It also can be considered
a high-level ``application-programmer-facing'' API similarly to SYCL,
thus offers constructs and overheads for programmer-productivity which are unneccessary for a portability layer.

\gls{hsa} is an open heterogeneous platform
specification that also defines a runtime API~\cite{HSA,HSART}.
A key differentiating feature of \gls{hsa} is that it standardizes on shared virtual memory,
making system-wide virtual memory addressing a required feature from
implementations. In hindsight this requirement was too much
too early, as system-wide virtual memory support is only recently appearing
in hardware and still usually requires explicit allocation or mapping
calls from the programmer.
For this work, \gls{hsa} could have been a valid choice for
a lightweight portability layer, but activity on the specification has ceased,
with mainly AMD using only selected parts of it in their software stack (the ROCr runtime component).

Another option to consider would be Vulkan~\cite{Vulkan} since it also
provides a compute pipeline stage which allows specifying general purpose compute
kernels. However, the
feature set of the compute kernels lacks some of OpenCL's features such as \gls{svm} which
would require further standard extensions.
It might be that in the future
the feature gap gets narrower and it would become a viable option. Meanwhile, layering OpenCL on top of Vulkan is an interesting option pursued by multiple open source projects to cover the devices which previously only had Vulkan driver support.

Level Zero~\cite{l0} is an API at a similar level of abstraction as \gls{hsa} and OpenCL.
However, it's currently only supported on Intel's devices, thus is not suitable for future-proof cross-vendor fat binaries.

Interestingly, OpenCL was originally created to provide an open cross-vendor alternative to the proprietary CUDA GPGPU (General Purpose GPU) programming model. \myhl{After its introduction in 2009} it has received wide support from hardware vendors, but some application developers are known to dislike it because it can be considered too low level as an application-facing API and unproductive with the driver and feature support lagging behind the proprietary alternatives.
However, as demonstrated in this paper, OpenCL can provide an excellent portability layer for implementing other higher-level programming models and APIs on top. This portability is due to multiple vendor's official support for the minimum feature set of OpenCL version 3.0 and to multiple long-maintained open source implementations. For these reasons, we have chosen OpenCL as the runtime through which to access the GPU.

\subsection{Device Program Representations}
\label{subsec:deviceProgramRepresentations}

Heterogeneous platforms suffer from the problem of device-side program description portability. There is a wide range of instruction-set architectures the kernels can target, and when the program is distributed in a binary form, the targets are known only at run time.
Thus, the choice of the format in which the device programs (kernels) are stored is critical as it should cover as many of the potential targets as possible.
Furthermore, the representation should be ``future-proof'' in a sense that the produced fat binaries could be made to run on entirely new platforms by only referring to the API specification.
At the time of this writing, there still seems to be no clear winning program representation in this regard. Various portable implementations of application-facing APIs resort to very fat binaries which store copies of the device program in multiple (virtual) instruction-set architectures to cover the various targets and offloading runtimes it might encounter at execution time. This is the case with~\cite{10.1145/3559009.3569687} and originally in AdaptiveCpp~\cite{10.1145/3529538.3530005}.

Recently AdaptiveCpp started storing kernels in the LLVM~\cite{LLVM} compiler \gls{ir} instead of storing multiple different binaries depending on the target. In this scheme, LLVM \gls{ir} is lowered to various target-dependent formats at runtime at the point when the target is known~\cite{OpenSYCLfatbin}.
This approach has benefits in comparison to storing abundance of device binaries in the another alternative, and works in theory, but it is also known that LLVM \gls{ir} is not supposed to be a portable program representation as it can embed target-specific intrinsics, has target specific data layout and endianness among other challenges.
LLVM \gls{ir} is not guaranteed to be stable across LLVM versions, which means that the fat binaries should have access to an LLVM library of version the \gls{ir} was generated with, which at worst requires to embed the LLVM library along and the required backends to the fat binary, forming an unnecessary dependency.
The problem of LLVM \gls{ir} not being target-independent nor stable across LLVM versions was attempted to be addressed by earlier \gls{spir} versions 1.2 and 2.0~\cite{SPIR2}: These first \gls{spir} versions were designed to support OpenCL C language kernels and were based on defined versions of LLVM \gls{ir}, which proved to be difficult to maintain long term.
LLVM-based \gls{spir} versions were later obsoleted in favor of the \gls{spirv}~\cite{SPIRV} format.
The goal for \gls{spirv} is to provide a robust cross-vendor specified intermediate language which is not affected by LLVM upstream changes and that shares specification effort with the Vulkan community.

\gls{hsa} specification defines an intermediate language called \gls{hsail} and a binary representation called BRIG~\cite{HSAIL}. A key technical difference between \gls{hsail} and \gls{spirv} format, is that \gls{hsail} has 
a fixed number of registers and an address space for spills unlike \gls{spirv}, which has infinite virtual registers due to being based on the \gls{ssa}~\cite{SSA} representation.
\gls{hsa} made a choice to not define a higher-level programming language (like OpenCL C) for the device programs, but only standardized a low level \gls{ir}.
As with the \gls{hsa} runtime specification, however, the activity on the \gls{hsail} specification has stalled.
There was also a GCC-based frontend for consuming BRIGs in a target-portable fashion, but after activity on \gls{hsa} quieted, the ``BRIG frontend'' was removed from the upstream GCC source code repository in a May 2021 commit.

In conclusion, while official \gls{spirv} OpenCL environment support from processor vendors is extensive as of this writing, it seems to be still the best option for a cross-platform representation given that it is an open standard defined democratically by multiple hardware vendors and is relied upon by OpenCL, Vulkan, and SYCL implementations among other use cases such as Direct-X adopting it.
In addition, thanks to open source tooling support available and useful \gls{spirv} producers such as \chipstar and Intel oneAPI \gls{dpc} appearing, the list of supported targets is expected to grow in the future. Therefore, we considered \gls{spirv} and OpenCL to be future-proof open standards on which to base the implementation.

\section{Implementing HIP/CUDA on OpenCL Runtime API}
\label{section:implementation}

The primary goal for \chipstar is to support the subset of CUDA features
as defined by HIP and expand the feature set beyond it whenever feasible
while relying on the chosen open standard APIs as much as possible.
In this section, we discuss how the OpenCL/\gls{spirv} specifications can be matched with
the commonly used features of CUDA/HIP and identify the most impactful gaps
that we believe should be covered in the future.

\subsection{Memory Model}

Due to their common history in GPGPU programming, CUDA/HIP and OpenCL share various
platform and memory model abstractions. For example, ``device memory'' is the same as
``global memory'' in OpenCL terminology (``shared'' is ``local'').
To avoid confusion in terminology we use only the CUDA/HIP terms in the rest of
this article. Similarly, we refer to the original CUDA versions when talking about
functions that have their counterparts in the HIP API.

A key difference between OpenCL and CUDA that required addressing is the
fact that CUDA implicitly infers the address space of the data in the device
program side whereas in OpenCL (before v2.0) the address space must be declared explicitly.
The CUDA's implicit address space inference is similar to the 'generic'
address space concept introduced in OpenCL v2.0, which was utilized to bridge
this gap.

The simplest interface in CUDA's host-side device memory management is \func{cudaMalloc()}.
\myhl{It returns a raw pointer to the targeted device's global memory, instead of an opaque buffer handle.}
% as is the case with OpenCL's basic buffer management functionality. This presents a small
%but significant difference from the OpenCL v1.x specification for device memory management;
%OpenCL v1.x only provides a buffer management API (\func{clCreateBuffer()} and others) which returns opaque \type{cl\_mem} handles.
%
%The opaque buffer handles cannot be used to implement CUDA device memory allocation
%because they do not provide access to the underlying raw device address or passing addresses in other data
%structures, which is allowed with the CUDA device pointers. 
\myhl{In order to implement these
capabilities, we utilized the }\gls{svm} \myhl{API that first appeared in the OpenCL v2.0.}
%The raw pointers vs buffers difference along with the
%implicit address space inference required us to lift the minimum OpenCL
%version to v2.0 to support even the most basic CUDA programs.

The \gls{svm} allocation API returns a raw pointer to a shared
virtual address space region. The ``\gls{cg} \gls{svm}'' variant can be used to
implement the basic device memory allocation. Mapping device memory allocation to \gls{cg} \gls{svm}
has a drawback that the device driver must support some of the unneeded \gls{svm} features such as
mapping the allocated regions to the virtual address space although just returning physical
device memory pointers would suffice. This means that the \chipstar implementation is actually
implementing CUDA's Unified Memory model by default. To alleviate the potential performance
impact of this, \chipstar can also use the Intel \gls{usm}
extension (\textit{cl\_intel\_unified\_shared\_memory}~\cite{intel-usm}), if supported by the runtime. \gls{usm} enables allocating strictly
device-only allocations, but still returns virtual pointers, which can be problematic for some implementations. 

\myhl{In order to provide a minimal allocation API} matching the basic \func{cudaMalloc()}'s needs \myhl{without requiring a }\gls{svm}\myhl{-capable OpenCL driver}, we introduced a new extension (\textit{cl\_ext\_buffer\_device\_address}\cite{intel-buffer}) that enables querying the raw device pointer of a cl\_mem allocation without needing to map the buffer to the same address range in the host's virtual memory. \chipstar can use any of these alternative memory management APIs, if advertised by the targeted OpenCL device.

CUDA provides an API to \textit{pin} memory so it's kept resident in the host memory and
optionally made accessible by devices from kernel code and is not swapped out to disk. 
The primary APIs to this functionality are
\func{cudaHostAlloc()} and \func{cudaHostRegister()}. The former allocates pinned
memory directly and the latter pins a previous host allocation. \func{cudaHostAlloc()}
is simple to implement with coarse grained \gls{svm} since by the coincidence of using
a shared virtual memory allocation, the buffers are by default accessible in both the host and
the device using the same pointer. However, the allocation might not be resident for the
duration of the execution, for example, if a CPU device is allowed to swap out such
allocations. However, that aspect can only be observed by the programmer as a performance difference.

\func{cudaHostRegister()} is a bit more challenging to implement on top of \gls{cg} \gls{svm} since it
allows registering a host address range to be a pinned region accessible both from the host and
the device \textit{after}
the host memory has been allocated. Since the allocation might not have
been originally allocated with the OpenCL SVM allocation API, but with a system memory allocator or even
from the stack, to implement correct functionality in this case, \chipstar creates
a shadow buffer using \func{clSVMAlloc()} and synchronizes it with the host region at
kernel start and end points. OpenCL 2.1 added a new \func{clEnqueueSVMMigrateMem()} API that enables fine grained specification of where regions of \gls{svm} are migrated, but it is not useful for this case since the source of \func{cudaHostRegister()} can be any host memory area whereas the API handles only \gls{svm} allocations.

The NVIDIA architectures post-compute capability 6 support on-demand page migration which
relies on the hardware memory management unit (page fault-based buffer migrations) for coherence
of the Unified Memory allocations. This frees the programmer from the need to perform explicit memory allocation and synchronization calls. This functionality maps to the Fine-Grained System \gls{svm} of OpenCL, but since hardware and driver support for fine-grain \gls{svm} is very rare at the time of this writing, on-demand page migration is not yet implemented by \chipstar.

\subsection{Tasks and Events}

The semantics of CUDA \textit{streams} and the ability to execute tasks/commands
asynchronously maps well to the \textit{command queues} of OpenCL. Each stream is expected
to execute commands in-order, which matches the in-order command queue semantics of OpenCL.
Commands are allowed to execute concurrently even within in-order command queues in OpenCL,
as long as the results are not observable from the outside, enabling concurrent kernel
execution~\cite{OpenCL}.

\subsection{Textures}

\chipstar supports only a subset of texture objects due to a limitation in OpenCL images. The notable differences between HIP/CUDA and OpenCL are that the texture objects are pointers to opaque C/C++ structures whereas in OpenCL/\gls{spirv} there is a special type per image dimensionality and that the texture objects can be loaded indirectly whereas OpenCL images can be only passed to kernels as kernel arguments. Therefore, some constructs such as the following cannot be expressed in \gls{spirv}:

\begin{verbatim}
  hipTextureObject_t Tx = ...;
  Ty Tv = cond ? tex2D<Ty>(Tx, X, Y) 
               : tex1D<Ty>(Tx, X)
\end{verbatim}

An LLVM pass is responsible for lowering texture object API based texture functions to OpenCL image fetches. The pass analyses endpoints of the texture objects by following their use-def and def-use chains. If the pass sees that a texture object is coming from a kernel parameter and it is only used by texture fetch calls for the same dimensionality, it will replace the texture object parameter with image and sampler parameters and translates the texture fetch calls with OpenCL image fetch calls of matching dimensionality which consume the new kernel parameter. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Compilation Aspects}
\label{section:compilation}

This section discusses the compilation flow used by \chipstar. We introduce the overall compilation flow, the device library implementation and summarize our findings on the key needs to extend the OpenCL/\gls{spirv} standards to bridge key feature gaps between the specifications.

\subsection{The Compilation Flow}

The offline compilation flow of \chipstar is built on the LLVM Project's~\cite{LLVM} Clang~\cite{Clang} frontend which provides the frontend language handling and splitting of the single source input to the device and host parts. The overall compilation process is shown in Fig.~\ref{fig:compilation}. It relies on the CUDA/HIP frontend of Clang, which was extended to produce \gls{spirv} binaries as an option to PTX or AMDIL for the device program. The LLVM \func{opt} tool is used to invoke special LLVM passes provided by \chipstar for lowering HIP features to the OpenCL-\gls{spirv} environment. The \gls{spirv} translation is performed using Khronos' LLVM-SPIRV-Translator tool~\cite{llvm-spirv}.

\begin{figure*}
    \centering
    \includegraphics[scale=1]{chipstar-compilation-v2.pdf}
    \caption{The construction of a HIP fat binary. Device code is compiled into LLVM \gls{ir}, chipStar applies a series of LLVM passes at optimization time. The resulting \gls{ir} is embedded into the binary and linked against the device library. }
    \label{fig:compilation}
\end{figure*}

Most of the compilation-related changes have been upstreamed to the LLVM project and very little compilation-related functionality remains within the \chipstar code base. The notable exceptions are compiler passes that handle CUDA vs. OpenCL differences in \func{printf()}, implement a device side \func{abort()} feature, handling of CUDA's device-side global variables, and an indirect memory access analyzer. The indirect memory access analyzer marks kernels that are known to not indirectly access allocations, which removes  unnecessary synchronizations for the majority of
benchmarks seen so far. Otherwise, due to CUDA's memory model, each launcher kernel can potentially access any previously allocated buffer, inducing significant unnecessary
data synchronization overheads in the common case where the kernels only access buffers set through their arguments.

\subsection{Lazy Just-in-Time Compilation}

Fig.~\ref{fig:online-compilation} shows the online compilation flow from \gls{spirv} to device code in the \chipstar runtime.
When a kernel launch is requested, the kernel function stub pointer is used to look up the associated \gls{spirv} module which, in turn, is \gls{jit} compiled to machine code.
To enhance runtime portability, the built-in library of the on-line device provides variations in built-in HIP functions for different device capabilities that are linked to the user’s device programs at runtime. For example, for HIP floating-point atomics the runtime chooses between an implementation that maps them to corresponding native functions via a \gls{spirv} extension or emulates them via atomic exchange operations.

\begin{figure*}
    \centering
    \includegraphics[scale=0.9]{chipstar-rt-compile-n-link.pdf}
    \caption{The just-in-time compilation flow. Once the program is executed, prior to calling main, a series of internal HIP calls are executed. These calls parse the fat binary and register the embedded \gls{spirv} files as modules. In lazy \gls{jit} mode, these modules are not compiled into kernels until their launch is requested.}
    \label{fig:online-compilation}        
\end{figure*}

%Initially, the kernel compilation was implemented in an eager manner in \chipstar: all of the kernels were compiled prior to entering main().  If the SPIR-V has a lot of kernels bundled in, but only a small subset of them are invoked by each application run, the compilation time could grow very high. For example, a neural network framework included all neural network operators in a single SPIR-V library, requiring its compilation at each network launch although only a subset of the operators were actually utilized.

\chipstar implements a lazy \gls{jit} compilation strategy \myhl{to avoid compiling all kernels of large }\gls{spirv}\myhl{ libraries such as those in neural network libraries, unless called from the host program.} 
%With lazy \gls{jit} enabled
%modules are not compiled immediately upon creation, but rather compilation is deferred until the first kernel call that requires the module. 
When compilation occurs, the \gls{spirv} binary is converted to the target format, build flags and options are set up, and backend-specific compilation methods (such as \func{clCreateProgramWithIL} for OpenCL) are used. 
Once compiled, modules are cached for future use. This approach significantly reduces startup time for applications with many kernels, as only the necessary kernels are compiled. 

The implementation includes performance monitoring features, tracking compilation time and logging whether modules are loaded from cache or compiled fresh.
The lazy \gls{jit} system is designed to be backend-agnostic, allowing different backends (Level0, OpenCL, etc.) to implement their own specific compilation strategies while maintaining consistent lazy compilation behavior. This flexibility enhances \chipstar's portability across various platforms and backends.

\subsection{Device Library}
The \textit{chipstar} device-side library implements the HIP math API, by using a combination of OpenCL C math built-ins, LLVM built-ins, OCML (part of ROCm-Device-Libs), and custom implementations.
Many of the functions in the HIP math API have an equivalent OpenCL built-in with adequate accuracy guarantees. However there are a few exceptions that cannot be mapped directly, and thus require software-based emulation such as floating-point atomics on some devices. The main challenge in terms of a fast yet portable implementation of the functions is due to differences in math accuracy requirements between CUDA/HIP and OpenCL. While CUDA provides very specific ULP accuracy requirements \cite{cuda-ulp} (most of which are higher than those in the OpenCL specification \cite{opencl-ulp}) for their math library, no such requirements were specified for HIP 6.1 which chipStar implements. These ULP requirements are now specified as of HIP 6.3 \cite{hip-ulp} but still seem to diverge from those required by CUDA.  

Furthermore, CUDA/HIP defines a set of \textit{intrinsics}, which are faster yet less accurate versions of the standard functions. 
This exposes a further difficulty when aiming for a portable, yet fast implementation: the level of accuracy achievable depends heavily on the targeted platform. Since CUDA is inherently meant not to be cross-vendor portable, the intrinsics are defined only to match the CUDA microarchitecture in an optimal manner, which might not be the case for other devices. 

OpenCL covers the use case of accessing fast but less accurate hardware operations by means of 1) a relaxed mathematics flag that can be enabled at device program build time and 2) with so-called native built-in functions in the built-in kernel API. Unfortunately, neither of these are usable for implementing the CUDA intrinsics by default due to not guaranteeing enough ULP accuracy as required by CUDA API. The relaxed math in OpenCL defines maximum rounding errors, but they are usually slightly less than what the CUDA intrinsics require Fig. \ref{ulp-math}. The OpenCL native built-in functions are an even worse fit for this use since they guarantee nothing of the accuracy but leave it entirely up to the implementation Fig.\ref{ulp-fast-math}. There is not even a possibility to query for the maximum error via a runtime API, the accuracy must be discovered via trial-and-error or from documentation of the hardware vendor. 

\begin{table*}
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Function} & \textbf{Intel Arc A770} & \textbf{Intel UHD 770} & \textbf{AMD gfx906} & \textbf{Intel i9-13900K} \\
\hline
cos               & 1                       & 1                      & 1                   & 2                         \\
exp               & 2                       & 2                      & 1                   & 1                         \\
log               & 1                       & 1                      & 2                   & 1                         \\
\hline
\end{tabular}
\caption{Maximum ULP Differences for Standard OpenCL Math Functions. OpenCL specifies ULP requirements of 4, 3, and 3 for cos, exp, and log, respectively.}
\label{ulp-math}

\end{table*}

\begin{table*}
\centering

\begin{tabular}{lcccc}
\hline
\textbf{Function} & \textbf{Intel Arc A770} & \textbf{Intel UHD 770} & \textbf{AMD gfx906} & \textbf{Intel i9-13900K} \\
\hline
native\_cos& 27.54                   & 27.54                  & 6.07                & 1084.87                   \\
native\_exp& 0.84                    & 0.84                   & 0.67                & 474.68                    \\
native\_log& 0.36                    & 0.36                   & 0.46                & 154.81                    \\
\hline
\end{tabular}
\caption{Average ULP Differences for Native OpenCL Math Functions. OpenCL does not have ULP requirements for native functions.}
\label{ulp-fast-math}

\end{table*}

Previous research \cite{ulp-comparisons-paper} shows significant disparities in the precision of GPU math functions, with CUDA consistently achieving the highest accuracy, followed by HIP, LLVM, and OpenCL (estimated based on relaxed math). CUDA’s functions maintain ULP errors close to 1, while HIP and LLVM exhibit progressively higher errors, sometimes exceeding 3-5 ULP. OpenCL’s relaxed math and native built-in functions lack strict accuracy guarantees, making it challenging to replicate CUDA’s precision without significant adjustments or corrections.

The ``correctness first'' principle mandates the use of arithmetics that have  guaranteed accuracy, which means to not receive any performance benefits of simplified implementations. This approach is not taken by \gls{dpc}, for example, as it defaults to a more relaxed floating point precision model (-fp-model=fast).
Achieving ULP requirements specified in the CUDA API would require emulating them in software . Performance impact would likely be too drastic to make \chipstar usable for high performance workloads.
Since we cannot make any guarantees about precision, we have chosen to map regular and intrinsic functions to the same OpenCL default accuracy function implementations.

We plan to optimize this aspect in the future via a new standard extension with a set of built-ins that guarantee the CUDA accuracy requirements to the application programmer while enabling the targeted platform to optimize and implement them as efficiently as possible.

\subsection{OpenCL Extensions}

The \chipstar compilation flow is built such that different advanced OpenCL features and extensions are not required from the target platform's driver or device unless the compiled input application specifically needs them. Although the minimal OpenCL 3.0 feature set plus coarse-grained \gls{svm} and \gls{spirv} consumption support covers a significant part of the most commonly used CUDA and HIP features, some functionalities require or can be improved with extensions to the OpenCL or \gls{spirv} specifications.

\begin{table*}[ht]
    \centering
    \begin{tabular}{|p{5 cm}|p{5cm}|}
    \hline
\textbf{Extension} & \textbf{CUDA/HIP feature(s)} \\
    \hline
cl\_intel\_unified\_shared\_memory & Used for optimized \func{cudaMalloc()} when available. \\
   \hline
cl\_ext\_buffer\_device\_address & Used for optimized \func{cudaMalloc()} when \gls{usm} nor \gls{svm} are available. \\
   \hline
cl\_intel\_required\_subgroup\_size & Used to fix the warp-size. \\
    \hline
cl\_khr\_fp64                       & If double precision floating point is used. \\
    \hline
cl\_khr\_subgroups                  & Warp-level synchronization with \func{\_\_syncwarp()}.\\
    \hline
cl\_khr\_subgroup\_ballot           & Warp-level ballot operations. \\
    \hline
cl\_khr\_subgroup\_shuffle          & Warp-level shuffle operations. \\
    \hline
    \end{tabular}
    \caption{OpenCL 3.0 standard extensions that \chipstar can use currently to implement CUDA/HIP features if the compiled application uses them.}
    \label{table:extensions}
\end{table*}

\begin{table*}[ht]
    \centering

    \begin{tabular}{|p{5 cm}|p{5cm}|p{5cm}|}
    \hline
\textbf{Extension (working title)} & \textbf{CUDA/HIP feature(s)} & \textbf{Status} \\
    \hline
cl\_ext\_alive\_only\_barrier       & A special work-group barrier for barrier calls which might not be reached by work-items that have exited the kernel as allowed by the CUDA's execution model. & Draft. \\
    \hline
cl\_ext\_cuda\_math     & Implement math functions and intrinsics with precision requirements that match CUDA's. To enable more optimized reduced precision intrinsics. & To be proposed.  \\
    \hline
cl\_ext\_device\_side\_abort        & Implement \func{\_\_trap()} on the low-level runtime side. The current implementation requires compiler transformations. & Public draft.  \\
    \hline
cl\_ext\_extended\_device\_properties & \func{hipGetDeviceProperties()} can be used to query more device properties than the basic OpenCL device or platform query APIs support, this fills the gap. & To be proposed. \\
    \hline
cl\_ext\_relaxed\_printf\_address\_space &  CUDA's \func{printf()}behavior with non-constant address spaces. Currently handled with compiler transformations. & Public draft. \\
   \hline   
cl\_khr\_command\_buffer            & For optimized implementation of CUDA graph re-execution. & Public. \\
    \hline
cl\_ext\_command\_buffer\_host\_data & For optimized implementation of CUDA graphs which transfer data between the host and the device. & Draft. \\
    \hline
cl\_ext\_command\_buffer\_host\_sync & For optimized implementation of CUDA graphs which synchronize with the host. & Public draft. \\
    \hline
cl\_ext\_subgroup\_id\_mapping & For forcing the desired thread id mapping when calling warp-level primitives that depend on the fixed warp size or the thread id ordering. but Luckily, in practical targets mapping is already the desired one by default. & Draft. \\
    \hline

    \end{tabular}
    \caption{Planned or drafted OpenCL 3.0 standard extensions that \chipstar might use in the future to implement CUDA/HIP features if the application uses them. The status column describes the state of the extension at the time of this article's publication. }
    \label{table:proposedExtensions}
\end{table*}




In Table~\ref{table:extensions} we summarize the standard extensions \chipstar can already utilize and which CUDA/HIP feature triggers their need. Table~\ref{table:proposedExtensions} describes further work-in-progress extension proposals we have identified to be useful for CUDA/HIP portability. These extensions are in different stages in the Khronos Group standardization process, which is noted in the table.\footnote{Note to reviewers: We will update the status for the final article version.}

Most of the extensions are relatively straightforward and the brief description in the table should suffice to grasp their purpose. However, the handling of warp-level primitives calls for a bit more thorough explanation:
One of the execution model differences between CUDA and OpenCL is that CUDA presents a finer grained fixed size grouping of the threads (OpenCL work-items) than the blocks (work-groups) called a \textit{warp}. In earlier CUDA versions, the threads in a warp could be assumed to execute in lock-step, implying that the enabled threads in the same warp would execute the same instruction. This implied that in some cases explicit synchronization could be omitted: In case of a usual read-modify-update case, the programmer could trust that the warp's threads all execute the read part before any of them proceeds to the update part, enabling in-place-updates without explicit synchronization. In later versions of the CUDA specification, the use of lock-step behavior in program logic was deprecated, but the feature has to be supported for legacy applications~\cite{cuda-lockstep}.

In addition to older CUDA programs potentially relying on the lock-step semantics to omit explicit synchronization, the fixed size warps (32 threads for NVIDIA and usually 64 threads in AMD devices) affect the execution semantics when executing warp-level functions that rely on the warp grouping and the mapping of the threads to the lanes of the warp.  Such primitives include the warp shuffles, which read data from a specific lane within the warp, and the explicit warp synchronization primitives.
The OpenCL specification, on the other hand, doesn't have a warp concept, but the work-items are free to make progress in any order and grouping. The specification, however, has a feature extension called ``subgroups'' that is used to implement the warp semantics in \chipstar when the kernel is detected to need it. However, in contrast to warps which have a specified form and content which allows the programmer to utilize them reliably, the basic subgroups of OpenCL are ``implementation-oriented''; they enable grouped execution in a manner that is simplest or most efficient for the driver and the hardware at hand. The sizes of the OpenCL subgroups are not fixed, but must be queried per kernel by the programmer in the basic extension. Also the way work-items are mapped to subgroup lanes so they can be referred to when using cross-lane intrinsics is also implementation-defined. To close the gap between subgroups and warps, an enhanced extension that \textit{forces} the subgroup size of the kernel to the desired size along with the linear id mapping is being proposed.

\subsection{Unsupported HIP/CUDA APIs}
\myhl{The following APIs have not yet been implemented as feature implementation timelines are driven by application requirements:Cooperative Groups,  Memory Pools, Inter-process Communication, Peer-to-Peer Access, and Occupancy APIs. Implementing some of these APIs might require additional OpenCL extensions. This is left for future work.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation}
\label{section:performance}

We evaluated  \chipstar  performance on various OpenCL-capable CPU and GPU platforms using a subset of the HeCbench benchmark collection~\cite{HeCbench}. All of
the results were produced using the \chipstar v1.2.1  release.

The HeCbench benchmark application selection criteria for each comparison was as follows:
\begin{enumerate}
    \item The application had the necessary API/language variations with a HIP version that could be built with \chipstar v1.2.1 and its ported libraries.
    \item For SYCL/CUDA comparisons, there must not have been significant identified performance-affecting structural or implementation differences between the SYCL/CUDA and HIP versions of the application. Some of the identified ``unfair differences'' were fixed and submitted to the HeCbench repository.  The exact branch used for benchmarking can be found here: https://github.com/CHIP-SPV/HeCBench/tree/chipStar-bench

    \item The application could verify its results and had to validate correctly on all platforms involved in the comparison.
    \item Applications that required hardware or OpenCL driver features that were missing or too limited on the platform were omitted. This mostly concerned the runs on embedded/integrated GPUs with limited memory or lack of double precision floating point support.
\end{enumerate}

\subsection{OneAPI \gls{dpc} on Intel oneAPI SDK}
\label{sec:SYCL-comparison}

For this evaluation we chose a subset of benchmarks included in the HeCbench suite and compared the performance of their HIP versions of the benchmarks against the SYCL versions. 
The SYCL versions were compiled with Intel's \gls{dpc} shipped with the oneAPI v2024.2.2 release. 

The following consumer-grade hardware from Intel, \myhl{Nvidia}, and AMD was used to perform the  benchmarking: Intel A770 discrete GPU, Intel UHD 770 integrated GPU, Nvidia RTX3060, AMD Vega VII and Intel i9-13900k CPU. 

Since both \chipstar and \gls{dpc} can use OpenCL as a backend, the evaluations are performed using the same OpenCL driver on the same GPUs. This  isolates the differences between the tested software stacks to the runtime and the LLVM \gls{ir} level device code compiler optimizations. 

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{chipStar-vs-sycl-speedup-truncated.pdf}
    \caption{\myhl{chipStar speedup over SYCL via} \gls{dpc} \myhl{with relaxed math, excluding 20 benchmarks where the performance difference was negligible }}
    \label{fig:best-vs-best}
\end{figure*}
In Fig. \ref{fig:best-vs-best}, 
we compare the expected fastest results between SYCL/\gls{dpc} and chipStar when floating point relaxations are enabled to the extent the implementations can utilize them. As expected, there are only minor differences in the performance since
 both \gls{dpc} and \chipstar implement their respective runtime by leveraging  OpenCL as the GPU API
and have a similar compilation flow: kernels are compiled into LLVM \gls{ir} which is then translated to \gls{spirv} which, in turn, is then \gls{jit} compiled to device code at runtime.  

For the vast majority of benchmarks, the performance difference between \gls{dpc} and \chipstar was negligible, but there are outliers to both directions. \gls{dpc} can currently benefit from FP relaxations more as it defaults to -fp-model=fast \cite{intel-fp-fast} which enables optimizations like floating-point reassociation, fused multiply-add operations, and the omission of certain intermediate rounding steps, as well as the use of less accurate device-side math functions. The HIP/CUDA equivalent is --use\_fast\_math \cite{cuda-fast-math} but \chipstar v.1.2.1 does not yet support this compiler flag.

We investigated the outliers and identified several reasons for the performance achieved:

\begin{itemize} {
    \item \textbf{Device library differences.} }There are significant differences in the device libraries: \gls{dpc} uses the Intel Math Functions (IMF) Device Library \cite{imf-website} whereas chipStar relies on a combination of LLVM built-ins, native OpenCL operations, a few custom implementations, and finally ROCm's OCML bitcode  implementations to provide complete coverage. Furthermore, \gls{dpc} defaulting to -fp-model=fast enables the use of \_native intrinsics which are significantly faster (and more accurate) than non-native implementations.
    \item \textbf{Effect of \gls{jit} Flags. } chipStar sees significantly higher performance increases when \gls{jit} flag  -cl-fast-relaxed-math is passed to the OpenCL runtime. 
    \item \textbf{Floating point relaxations}. \gls{dpc} defaults to -fp-model=fast which results in more aggressive optimizations by removing IEEE 754 guarantees and assuming no NaN/Inf values, generation of FMAs. In comparison, HIP allows only FMA contractions by default.

    \item \textbf{Value assumptions.} \gls{ir} produced by \gls{dpc} takes advantage of AssumeTrueKHR which is a \gls{spirv} instruction that tells the compiler to assume a given condition is always true, allowing it to optimize code more aggressively under that assumption. Using this instruction should give it a performance advantage \myhl{over} chipStar. 
    \item \textbf{Intel-specific decorators.} \gls{dpc} tends to produce \gls{ir} which often contains Intel-specific operations that are meant to help with optimization when targeting Intel platforms which support the extensions, such as OpAliasDomainDeclINTEL, OpAliasScopeDeclINTEL whereas chipStar \gls{ir} is more generic.

\end{itemize}

One thing to note is that the following results include only the benchmarks which passed correctness checks and in this regard chipStar ended up successfully running 6 \myhl{fewer benchmarks} compared to SYCL. A lot of these benchmarks have quite tight epsilon bounds so it's likely that with small adjustments the number could be reduced.

In order to analyze the benefits of IMF, we modified the chipStar device library to link against IMF for exp and sqrt calls and tested the performance again which resulted \myhl{in the} adam benchmark going from 0.16x to 0.50x.

After we applied the -cl-fast-relaxed math \gls{jit} flag, all of these benchmarks performed equally well in \chipstar and \gls{dpc}. In conclusion, all the cases where chipStar is slower than \gls{dpc} can be explained by the differences in math device library and the more aggressive default optimization of the \gls{ir} in terms of floating point calculations due to \gls{dpc} defaulting to -fp-model=fast.

\myhl{We selected} 9 benchmarks where chipStar-v1.2.1 outperforms \gls{dpc} \myhl{for closer examination}. Of these, most significant difference is seen in gaussian. Upon \myhl{further examination}, the time spent inside kernels is identical between these two runtimes so the most likely reason for the difference would be \gls{dpc} overhead. This overhead cost could be comprised of either \gls{dpc} startup costs, \gls{dpc} kernel invocation costs or a combination of both.
\textbf{}
To test for this, we isolated the outperforming benchmarks and performed a \myhl{scaling} experiment by comparing performance on the original problem size to the performance achieved with the number of iterations \myhl{or problem size increased by 5x} (Fig.~\ref{fig:chipstar-outperform-sycl-1x}). \myhl{This scaling leaves the runtime of a single kernel unchanged, which} isolates the \gls{dpc} startup overheads.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{chipStar-vs-sycl-scaled.pdf}
    \caption{\myhl{chipStar speedup over SYCL via} \gls{dpc} \myhl{at 1x and 5x problem sizes}.}
    \label{fig:chipstar-outperform-sycl-1x}
\end{figure*}

Scaling the number of iterations reduced the geomean chipStar-v1.2.1 speedup from 1.31x to 1.17x indicating that \gls{dpc}
has significantly higher startup costs compared to chipStar.

We analyzed the cases with the most dramatic differences and identified various explanations:
Many of the benchmarks executed very short kernel commands, making the benchmark actually mostly measure the host API call execution speed.
For example, the ``overlay'' benchmark could be sped up significantly by switching off the profiling command queue feature. 
In some cases the device built-ins were more optimized in \chipstar than in \gls{dpc}, in some cases it was the opposite.
For example, when we compared the \chipstar and \gls{dpc} LLVM \gls{ir}s of the device code for the ``nlll'' benchmark, we found that only \chipstar performed the if-conversion optimization that converts some of the very small branches to conditional moves, which provided significant benefits.

In conclusion, \chipstar has lower startup costs than \gls{dpc} and either slightly outperforms or matches \gls{dpc} performance across a variety of benchmarks \myhl{though} there are some exceptions in either direction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{CUDA on NVIDIA CUDA SDK}

It is interesting to compare the performance of CUDA programs compiled and ran using the NVIDIA's proprietary CUDA versus \chipstar over an OpenCL runtime. In this experiment, we first compiled applications using the CUDA SDK 12.4 to get a baseline. The same benchmark cases (the CUDA versions) were then compiled using \chipstar to the portable fat binary that uses OpenCL as the portability layer which was then run on rusticl/zink, an OpenCL implementation on top of NVidia's proprietary Vulkan driver. The execution time when compiling using strict maths was measured on an NVIDIA RTX 3060 GPU with the results shown in Fig.~\ref{fig:rtx3060-cudasdk-vs-rusticl}. The geometrical of 1.01 tells that the overhead of the chipStar, the OpenCL API and the rusticl OpenCL runtimes is negligible on average. A few outlier cases vary to one direction or another.

\begin{figure*}
       \centering
       \includegraphics[width=1\linewidth]{cuda-vs-rusticl-truncated.pdf}
      \caption{\myhl{chipStar via rusticl/zink stack, speedup over CUDA SDK excluding 36 benchmarks where the performance difference was negligible}}
      \label{fig:rtx3060-cudasdk-vs-rusticl}
\end{figure*}

\subsection{HIP on AMD ROCm}

Since \chipstar can be viewed as a more portable implementation of HIP, it is interesting to compare its speed against the original HIP implementation from AMD. For this comparison, we utilized the HIP compiler and runtime from the AMD ROCm package version 6.2.4 as a baseline to compile and execute a set of HeCbench HIP benchmarks on an AMD Radeon Pro VII GPU. To run the \chipstar fat binaries on the same GPU, we used the rusticl OpenCL implementation on the radeonsi driver. AMD's OpenCL implementation does not support \gls{spirv} input at the time of this writing, preventing its use in this comparison for running the \chipstar binaries.

We have already demonstrated that the \chipstar runtime does not introduce additional overheads in Fig. \ref{fig:best-vs-best} so the following performance differences are a product of rusticl, not \chipstar. Rusticl is still in the development phase and is not yet optimized for all targets, which explains the much larger performance differences in Fig. \ref{fig:radeonprovii_rocm_vs_rusticl} compared to Fig. \ref{fig:best-vs-best}

Given the current state of rusticl, there is not much value in doing an extensive performance anomaly studies. However, one such anomaly is `pnpoly' which is more than 2.5x faster on chipStar as can be seen in Fig. \ref{fig:radeonprovii_rocm_vs_rusticl}. The reason is that ROCm is slightly faster for tile sizes smaller than 32 and for larger ones notably slower. Unfortunately for ROCm, the benchmark tracks times for the largest tile which happens to be the slowest one on ROCm. 

\begin{figure*}
      \centering
      \includegraphics[width=1\linewidth]{rocm-vs-chipstar.pdf}
      \caption{\myhl{chipStar  via rusticl/radeonsi stack speedup over ROCm. Running on AMD Radeon Pro VII.}
      }
      \label{fig:radeonprovii_rocm_vs_rusticl}
\end{figure*}

\subsection{Portability Testing}

In order to test the extent of portability of the runtime API layer based on the OpenCL standard, and to verify that offloading to a GPU via \chipstar can bring speedups in comparison to running on the host CPU, we compiled and executed sets of HeCbench applications on various platforms which included both a CPU and a GPU with a capable enough OpenCL support to execute the same compiled \chipstar fat binary on both devices. Given the experimental state of some of these platforms,  the following performance numbers are not indicative of the true hardware potential and thus should be interpreted solely as indicators of application portability and not performance. The platforms and their results are presented below.

\paragraph{RISC-V CPU \& PowerVR GPU:} In this experiment we utilized the VisionFive2 single board computer for building and running the benchmarks. PoCL~\cite{PoCL} was used for running the benchmarks on the CPU and the proprietary OpenCL driver from Imagination Technologies was used for the GPU.  The results are visualized in Fig.~\ref{fig:intel-visionfive2-gpu-cpu}. The lower performance (0.74 geom mean) of the PowerVR GPU vs RISC-V CPU can be explained by the GPU having much less on-chip resources than most benchmarks could utilize. 
The GPU also has a native workgroup size of only 32 (subgroup size 16), while most HeCBench benchmarks use a workgroup size ranging from 128 to 1024, leading to additional thread context switches. Due to not being able to force the subgroup size to match the warp width in this platform also prevented some of the benchmarks from running. Furthermore, the GPU's limited local memory is used also to store images, samplers, the OpenCL constant data and pointers to global memory - in addition to the shared data of the application kernels~\cite{PowerVRPerfGuide}. The memory limitations and the lack of fp64 support were the main reason some of the test cases were not running at all on this platform.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{riscv.pdf}
          \caption{\myhl{chipStar on PowerVR GPU, speedup over RISC-V CPU. }}
    \label{fig:intel-visionfive2-gpu-cpu}
\end{figure*}
% \paragraph{Intel(R) Core(TM) i9-13900K} This platform has a 24-core 13th Gen Intel(R) Core(TM) i9-13900K and an Intel UHD Graphics 770 integrated GPU. OpenCL on the CPU and the GPU were supported by Intel's OpenCL drivers  24.35.30872.22. The results are shown in Fig.~\ref{fig:intel-i9-cpu-vs-igpu}.  It's important to note that the i9 CPU, having 24 cores, is quite powerful compared to the iGPU so we don't necessarily expect a significant offloading speedup in this case, especially when the OpenCL CPU driver is able to vectorize OpenCL work-items across the SIMD lanes of the CPU. However, as expected, most of the test cases run on this platform and are significantly faster on the iGPU, validating the portability as well as the offloading benefits.

% \begin{figure*}[tb]
%       \includegraphics[width=1\linewidth]{Figures-2025/igpu-vs-cpu.pdf}
%       \caption{Speed on Intel UHD Graphics 770 iGPU speed normalized to i9-13900K CPU.} 
%       \label{fig:intel-i9-cpu-vs-igpu}
% \end{figure*}

\paragraph{ARM Cortex A53+A73 CPU \& Mali G52 GPU} For the CPU, PoCL~\cite{PoCL} was used as the OpenCL driver while the GPU was supported by the ARM's proprietary OpenCL driver OpenCL C 3.0 v1.r40p0-01eac0.06c59e7df4d178b1ae2ad8082e91ad02. The results are shown in Fig.~\ref{fig:mali-vs-cortex}. A lot of applications were not able to run because of limited memory in the GPU and lack of double precision floating point support. However, various benchmarks showed significant benefits from CPU to GPU offloading, as expected.

\begin{figure*}
      \centering
      \includegraphics[width=1\linewidth]{arm.pdf}
      \caption{\myhl{chipStar on ARM Mali GPU speedup over ARM Cortex CPU.}}
      \label{fig:mali-vs-cortex}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{HPC Application Case Study: GAMESS-GPU-HF}
\label{section:applications}

In order to further test \chipstar in practice, we ported a
complex HIP/CUDA-based HPC application and its dependency library using \chipstar. The test environment for the experiment was the Aurora supercomputer utilizing Intel Datacenter Intel® Data Center GPU Max Series (referred to from here on as PVCs, as in Ponte Vecchio) as the accelerator part~\cite{aurora}.

\subsection{GAMESS-GPU-HF}

General Atomic and Molecular Electronic Structure System (GAMESS~\cite{gamess,gamess2}) is a quantum chemistry software package which implements many electronic structure methods. 
The code base is primarily in Fortran 77/90 with some C/C++ and a CUDA library. Recently a new GPU version of the Hartree-Fock (HF) and RI-MP2 methods were implemented in CUDA which scales to 4096 nodes on Summit, an Nvidia V100-based supercomputer \cite{gamess_cuda1, gamess_cuda2, summit}.
In this porting case we focused on the Hartee-Fock (HF) algorithm used by a CUDA library in GAMESS described in \cite{gamess_cuda1}, which has been ported to HIP. The HF method is a common quantum chemistry method which is often the starting point for other higher-accuracy methods.

The HF method primarily involves the computation of $N^4$ two electron integrals (where $N$ is a measure of molecular system size) as well as matrix contractions of the two electron integrals once they are formed, and computation of eigenvectors.

The two electron integrals are implemented as HIP/CUDA kernels which were optimized for Nvidia GPUs. The kernels total over 20,000 lines of HIP/CUDA  code. The matrix contractions and eigensolves are done on the GPU via calls to the HIP math libraries hipBLAS and hipSOLVER.

Since the application uses ROCm software platform libraries hipBLAS and hipSOLVER, they needed to be ported as well. The required interfaces of these libraries were implemented for Intel hardware by using Intel oneMKL as a backend. This is done with two layers: first, a shim library, H4I-MKLShim \cite{mkl_shim}, which provides shims for the SYCL-based oneMKL functions. This is designed to be used by other libraries which wish to call the oneMKL functions from a different API, such as hipBLAS, to use Intel GPUs. The chipStar project currently implements hipBLAS, hipSOLVER, and hipFFT in the libraries H4I-HipBLAS \cite{chipBLAS}, H4I-HipSOLVER \cite{chipSOLVER}, H4I-HipFFT \cite{chipFFT}, respectively. These allow calls to hipBLAS, hipSOLVER, and hipFFT functions to run on Intel GPUs.

In terms of functionality, the HF code compiles and was verified to run correctly with \chipstar on PVCs. The porting effort was relatively low, with one exception due to a small but significant specification difference in CUDA vs. OpenCL related to kernel thread synchronization: In CUDA group barriers are not counting in exited threads, meaning that there can be early returns from the kernel by a subset of the threads after which it is still legal to perform barrier synchronization with the remaining subset -- the exited threads are just not counted in. In OpenCL this case is undefined behavior and in many implementations can lead to a deadlock. To tackle this gap, an OpenCL extension adding a group barrier with similar semantics would be needed (see \textit{cl\_ext\_alive\_only\_barrier} in Table~\ref{table:extensions}).

\subsubsection{Performance}

The performance of the HF code was measured by compiling and running the same HIP source code on a PVC through \chipstar (Release 1.2.1), an Nvidia A100 through CUDA 12.2.2 with ROCm 6.0.0, and an AMD MI250 through ROCm 6.3.0. To be clear, this is comparing \chipstar on the PVC GPU to native HIP and CUDA on the Nvidia and AMD GPUs. 
To investigate the performance, a HF energy computation of a cluster of 150 water molecules with a STO-3G basis set was run 10 times on PVC, A100, and MI250. The average and standard deviation of the runtimes are displayed in Table~\ref{table:gamess_perf}.

Table~\ref{table:gamess_perf} shows that the total HF energy calculation time (the SCF time) on the Nvidia A100 is shortest (1.66 s) and on one GCD of an AMD MI250 is the longest (4.71 s). The Intel PVC time, through \chipstar with the OpenCL backend, is 2.8s, about 1.7x the runtime on the A100 GPU. From comparing the memory bandwidth and peak double-precision floating point operations possible on an A100 \cite{a100_measured} and a PVC stack \cite{applencourt2024ponte}, we see that we expect memory-bound codes on a PVC stack to take about 1.3x (1.3 $\frac{TB}{s}$ / 1.0 $\frac{TB}{s}$) the time on an A100 and compute-bound codes on a PVC stack to take about 0.56x (9.4 $\frac{TFlop}{s}$ / 17.0 $\frac{TFlop}{s}$) the time on an A100. Note that the 1.3x and 0.56x are upper bounds for performance since not all the runtime is on the GPUs, and the GPU library is a complex code with multiple asynchronous kernels and memory copies. A full performance analysis is out of the scope of this paper. 

Although the Intel PVC time through \chipstar is 1.7x slower than the A100 time, this is not too far from the 1.3x and 0.56x upper bound expectations based on hardware comparisons. The runtime on a PVC stack with \chipstar is competitive with the runtime on other architectures and roughly within expectations based on the compute- and memory-peak comparisons of a PVC stack and an A100. Thus we expect HIP applications currently running on Nvidia and AMD GPUs to run and perform reasonably well with \chipstar on Intel GPUs.

\begin{table}[t]
\centering
\begin{tabular}{cccl}
\cline{1-3}
 &
  \begin{tabular}[c]{@{}c@{}}Average SCF Time (s) \\ (Ratio over\\ Nvidia A100)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Standard \\ Deviation\end{tabular} &
   \\ \cline{1-3}
Nvidia A100          & \begin{tabular}[c]{@{}c@{}}1.66\\ (1x)\end{tabular} & 0.01                 &  \\ \cline{1-3}
\begin{tabular}[c]{@{}c@{}}AMD MI250\\ (one GCD)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}4.71\\ (2.8x)\end{tabular} &
  0.01 &
   \\ \cline{1-3}
\begin{tabular}[c]{@{}c@{}}Intel PVC\\ (one Stack,\\ OpenCL)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2.84\\ (1.7x)\end{tabular} &
  0.03 &
   \\ \cline{1-3}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{}                                & \multicolumn{1}{l}{} & 
\end{tabular}
    \caption{Timing comparison for SCF, Fock, and DIIS times (s) for GPU integral code across Intel, AMD, and Nvidia. Average over 10 runs.}
    \label{table:gamess_perf}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{section:relatedWork}

The origin of \textit{chipstar} is on the HIPCL~\cite{HIPCL} prototype which first tested the concept of compiling HIP programs to fat binaries relying on OpenCL and \gls{spirv}. The \chipstar tool described in this article is a result of an almost a complete rewrite of the HIPCL code base and over approximately three years of continuous development work by multiple partners and HPC users. The HIPCL code base was initially forked to a separate code base to utilize the Level Zero~\cite{l0} low level API directly (HIPLZ~\cite{HIPLZ}) after which the OpenCL backend of HIPCL and the Level Zero backend of HIPLZ were merged to the same code base discussed in this article.
A large number of missing essential features have been implemented since the initial prototypes were published. This article thus significantly expands upon the original poster abstract that introduced the early-prototype-stage HIPCL and now presents a much more mature software stack usable for a wider range of real-world workloads.

When comparing \chipstar to other HIP implementations, obviously the original ROCm, the AMD's official GPU software platform~\cite{ROCm}, is the baseline. ROCm consists of the general purpose programming API compilation and runtime support for HIP, and a set of libraries that support different degrees of compatibility with the CUDA platform. \chipstar is not a new backend in addition to the AMD GPU and NVIDIA GPU backends provided by the AMD's offering, but has an important technical difference: \chipstar aims to offer runtime portability by its open standard based fat binary, removing the need to recompile the input software per target vendor platform, which is the case with ROCm.

SYCLomatic~\cite{SYCLomatic} is a tool contributed by Intel Corporation for converting CUDA sources to the cross-vendor open standard SYCL~\cite{SYCL}. Similar to AMD's HIPify, but in contrast to \chipstar which aims for source-level compatibility, SYCLomatic is a source-to-source conversion tool, which has its good and bad sides. The most apparent implication of relying on source-to-source conversion is more about maintenance aspects than technical ones; it neccessitates the further development of the converted application to proceed using the SYCL API instead or in addition to CUDA. The main drawback is that in reality many code bases are difficult or impossible to convert solely to SYCL without having the CUDA version as a backup due to legacy, risk-management or technical reasons. The main benefit is that SYCL is an open standard, in constrast to CUDA, enabling more fair competition ground between hardware vendors. Thus, being able to target many platforms from a fat binary compiled from the unmodified CUDA/HIP source code base using a \chipstar-style open platform approach can have its benefits. Furthermore, since \chipstar is not a linkage-time or binary translation solution, but requires recompilation, it coincidentally also encourages utilizing and further developing the cross-vendor ecosystem APIs it relies upon. Furthermore, as of this writing SYCLomatic supports only CUDA, not HIP, while HIP has an increasing number of new applications implemented directly using it.

ZLUDA~\cite{ZLUDA} is a tool for running unmodified CUDA binaries on AMD GPUs. It works by reimplementing the \cuda driver API, and converting NVIDIA PTX~\cite{ptx} to the vendor-specific \gls{ir}s. Since it is a ``drop-in solution'' that works at program loading/linkage time, it can execute unmodified CUDA fat binaries, which is very comfortable to the end users as it doesn't require access to the source code of the application. While we see ZLUDA as an excellent tool, it requires reverse engineering CUDA SDK's binary interfaces and keeping up-to-date with the NVIDIA PTX as it evolves. We believe, in the longer term, especially as more of the missing extensions we describe are adopted by OpenCL implementations, \chipstar can provide a more robust solution. 
In addition, ZLUDA also doesn't support HIP as an input and now only targets AMD GPUs, whereas a key goal of \chipstar is extensive cross-vendor portability.

MCUDA~\cite{MCUDA} is the oldest tool we found for porting CUDA programs to non-NVIDIA platforms. MCUDA does source-to-source translation of kernels in a fashion that the translated kernels can execute efficiently on CPUs on a single CPU thread while respecting the barrier synchronization. In the case of \chipstar, since it uses OpenCL as its portability layer, it can similarly target also vectorized CPU execution through CPU-targeting OpenCL implementations such as the Intel OpenCL CPU driver and PoCL's CPU drivers~\cite{PoCL}. Both of them are capable of vectorizing work-items (CUDA/HIP threads) inside work-groups, which translates to implicit autovectorization of CUDA/HIP kernels across CUDA threads and provide the benefits of CPU execution such as easier kernel debugging.

Swan~\cite{Swan} is another early source-to-source tool for CUDA porting. It generates OpenCL code from CUDA, providing similar level of portability as \chipstar does. Another similar tool, CU2CL~\cite{CU2CL} was published in the same year as \cite{Swan}. Neither Swan nor CU2CL are maintained any longer.  In comparison to \chipstar, the main technical differences to these tools are that \chipstar utilizes the latest version of the OpenCL standard to support the newer CUDA/HIP features, uses \gls{spirv} as the intermediate language (no need to generate textual OpenCL C with its limitations) and it doesn't suffer from problems related to source-to-source translations as \chipstar provides source-level compatibility.

The closest comparable CUDA porting tool we could find is CUDA-on-CL~\cite{CUDAonCL}. Like \chipstar, it similarly compiles CUDA programs using Clang/LLVM-based compiler chain to binaries which then execute on OpenCL platforms. However, similarly to Swan and CU2L, it compiles device kernels to OpenCL C whereas \chipstar uses \gls{spirv} as the portable binary format. Other technical differences in \chipstar are related to the use of modern OpenCL standard features to implement some of the features of CUDA. These include using \gls{svm} to implement raw pointers and implementing warp-level primitives such as shuffles using the subgroup features. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{section:conclusions}

In this article, we presented \chipstar, a compilation flow and a runtime for CUDA/HIP applications using open cross-vendor supported standards. In comparison to previous tools, \chipstar's goal is on source-level compatiblity which we believe has longer-term robustness benefits in comparison to binary translation. 
It relies on open standards at the portability layer level which in our opinion has significant inherent far-reaching value.

The performance of HIP benchmarks using \chipstar was shown to be on par or surpass their SYCL/\gls{dpc} versions. We've also shown that \chipstar is very competitive against CUDA running on Nvidia GPU with comparisons to HIP and AMD hardware being less favorable due to OpenCL driver options on AMD hardware.

An example of the source-level compatibility was provided with GAMESS-GPU-HF, a code base with a significant number of kernel code lines. This demonstrates that \chipstar is a useful option for applications that are not feasible to port to more cross-vendor supported open standard input APIs such as SYCL or OpenMP.

\myhl{In the future, we will focus on expanding the HIP/CUDA feature coverage such as Graphs API and collaborative workgroups in} \chipstar.\myhl{ We will also expand the set of supported core libraries (hipFFT, hipSPARSE, etc) in the CUDA and HIP ecosystems to enable more real-world applications.}

%\myhl{In the future, }\chipstar\myhl{ will focus on expanding the HIP/CUDA feature coverage such as Graphs API and collaborative workgroups, while expanding the set of supported core libraries (hipFFT, hipSPARSE, etc) in the CUDA and HIP ecosystems to cover more real-world applications. }

\begin{acks}

This research was supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration.

This work was supported by the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357. We also gratefully acknowledge the computing resources provided and operated by the Joint Laboratory for System Evaluation (JLSE) at Argonne National Laboratory.

This manuscript has been coauthored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.  The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (\url{http://energy.gov/downloads/doe-public-access-plan}).

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.

This research used resources of the National Energy Research Scientific Computing Center (NERSC), a Department of Energy Office of Science User Facility.

Tampere University authors' contributions were in part funded via the DARE SGA1 Project, which is an European High-Performance Computing Joint Undertaking (JU) under Grant Agreement No. 101202459. The JU receives support from the European Union’s Horizon Europe research and innovation programme and Spain, Germany, Czechia, Italy, Netherlands, Belgium, Finland, Greece, Croatia, Portugal, Poland, Sweden, France and Austria.

\end{acks}
%\printglossary[type=\acronymtype]
\input{chipstar.acr}

\bibliographystyle{SageV}
%\bibliography{chipstar}
\input{chipstar.bbl}

\end{document}
