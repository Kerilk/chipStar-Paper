\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{color}
\usepackage{amssymb}

\usepackage{ifthen}
\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
{ \newcommand{\mynote}[3]{
     \fbox{\bfseries\sffamily\scriptsize#1}
        {\small$\blacktriangleright$\textsf{\emph{\color{#3}{#2}}}$\blacktriangleleft$}}
  \newcommand{\newtext}[1]{{\color{orange}{#1}}}}
{ \newcommand{\mynote}[3]{}
  \newcommand{\newtext}[1]{#1}}

\newcommand{\pj}[1]{ \mynote{PJ}{#1}{blue} }
\newcommand{\bv}[1]{ \mynote{BV}{#1}{green} }

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{hipstar: Enabling HIP and CUDA on an Open Cross-Vendor Standard Software Stack}

%\author{pekka.jaaskelainen }
%\date{March 2023}

\author{Pekka Jääskeläinen, Henry Linjamäki, Michal Babej, Peng Tu, Sarkar Sarbojit, Brice Videau, Colleen Bertoni, Kevin Harms, Paulius Velesko, Phil $?$, Rahul $?$, Jisheng Zhao, Jeffrey $?$, Vivek $?$...,
        % <-this % stops a space
\thanks{Pekka Jääskeläinen, Henry Linjamäki, Michal Babej, Peng Tu and Sarbojit Sarkar are with Intel Corporation. \textit{Corresponding author: Pekka Jääskeläinen, email: pekka.jaaskelainen@intel.com}.}
\thanks{Pekka Jääskeläinen is also with Tampere University, Finland. }
\thanks{Paulius Velesko is with Pagan LC.}
\thanks{Brice Videau, Colleen Bertoni and Kevin Harms are with Argonne National Laboratory, ...}
\thanks{Phil X is with Oak Ridge National Laboratory, ... }
\thanks{Rahul Y is with National Energy Research Scientific Computing Center.}
\thanks{Jisheng Zhao, Jeffrey $?$ and Vivek $?$ are with Georgia Institute of Technology Atlanta, Georgia.}
\thanks{\pj{The authors are not in any particular order. I put myself as 1st author as I'm leading the writing, and then I ordered the co-authors according to their affil.}}
%\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{IEEE Transactions on Parallel and Distributed Systems,~Vol.~X, No.~Y, Month~YEAR}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

%Well it depends on the kind of paper you want to write, but if we present CHIP-SPV with it's environment, we need to have Phil (ORNL) for his work on hipBLAS (and maybe Wael, but his contributions are more limited), we need to have the work done on applications by Colleen and Rahul (NERSC), and we need to have Jisheng that wrote the prototype and Jeffrey (both GaTech) that designed the initial interrop. I think you can count on contributions from most of these.
%Of course, we'll have the whole team at intel and Paulius. I doubt AMD will want to be on.
%In the end it could very well be a Journal paper, depending on the above involvement in the text.
%Once here, we only have a couple more seniors who may want to join: Vivek (GaTech), Kevin (ANL), who were on the original hiplz paper. They both did participate in the writing.
%Since we're not talking about CUDA I assume Hyesoon and her team will want to do a separate paper

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

%This document describes the most common article elements and how to use the IEEEtran class with \LaTeX \ to produce files that are suitable for submission to the IEEE.  IEEEtran can produce conference, journal, and technical note (correspondence) papers with a suitable choice of class options. 

Due to NVIDIA dominating the GPU market, despite its lack of cross-vendor portability, the C/C++-based application programming interface of CUDA and its related key libraries are still used in a significant fraction of software utilizing GPU-based acceleration. AMD's ROCm and its Heterogeneous-compute Interface for Portability (HIP) aims to alleviate the CUDA's lack of portability by providing a route out from the NVIDIA CUDA platform to AMD's devices.
  
In this article we describe \textit{hipstar}, an open source software platform which allows running CUDA and HIP programs on a open cross-vendor  standard based software platform based on OpenCL, {SPIR-V}, and a carefully crafted set of standard extensions. The solution is usually ``drop-in'', meaning no application modifications are typically required. We discuss the relevant technical aspects of \textit{hipstar} related to the feature mismatches between CUDA/HIP and OpenCL and exemplify its runtime overheads in comparison to executing CUDA/HIP applications directly with NVIDIA's CUDA software platform. The measurements show that the overheads induced by \textit{hipstar} are typically only in the order of N-M\%, thus negligible.\pj{TODO} Although being a relatively young code base, \textit{hipstar} is now considered mature enough for production use, as demonstrated by multiple application porting case studies performed for the Aurora supercomputer.

\end{abstract}

\begin{IEEEkeywords}
CUDA, HIP, OpenCL, SPIR-V, Portability, Shared Virtual Memory
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\IEEEPARstart{W}{alled} garden strategy is popular within market dominating companies. Its purpose is to lock-in customers to company's products by making escaping the gates of the garden as costly as possible. NVIDIA's CUDA software platform is considered to be one of such walled gardens. It in part helps NVIDIA to expand and keep a foothold of their GPU market advantage, and at the same time maintain high innovation pace on the software APIs since there is no need to work with standardization committees that always have to aim for a consensus among multiple participating vendors.

For the end-users and other hardware vendors, naturally, the situation of a single-vendor dominating API is not optimal. End-users benefit from open standard software interfaces that enable switching the hardware without incurring significant non-recurring engineering costs required for porting all the legacy applications and libraries to a new software platform just to be able to utilize the purchased hardware optimally. Similarly, other hardware vendors aiming to get their piece of the market pie, would prefer an API that is not dictated by a single vendor.

AMD's ROCm~\cite{ROCm} software platform and its Heterogeneous-compute Interface for Portability (HIP) language~\cite{HIP} helps escaping the CUDA walled garden by providing a route out from the NVIDIA CUDA platform to AMD's devices. HIP defines a subset of CUDA that is more easily portable to various hardware, thanks mainly to omitting various advanced features available in the later CUDA versions (some of these features are discussed in Section~\ref{subsection:compatgaps}). In order to enable easy automated path from CUDA applications, HIP is largely a copy of a CUDA C/C++ subset with a few minor differences, and renamed function names. It alleviates the CUDA portability problem, but doesn't solve it satisfactorily due to AMD targeting primarily their self-specified low level ROCm APIs which have not been so far ported to other than AMD platforms. An open source CUDA/HIP software platform solely based on open standards with the sincere aim for cross-vendor portability is still lacking.

In this article we propose a complete open source software platform that enables porting applications from NVIDIA-driven CUDA and AMD-driven ROCm platforms to any platform supporting the cross-vendor open standards OpenCL and SPIR-V. We evaluate the portability aspects on platforms with proprietary drivers, and as a further contribution, extend an open source OpenCL implementation to expand the scope to further device types. In comparison to the previous solutions, our proposed platform enables usually source-modification-free compilation of HIP programs along with supporting the necessary library dependencies for the most essential CUDA/ROCm APIs. 

The rest of the article is organized as follows: Section~\ref{section:relatedWork} overviews the related work to the proposed software platform and its components. Section~\ref{section:runtime} discusses the technical challenges related to the runtime execution. Section~\ref{section:compilation} describes the most relevant parts of the compiler support. Section~\ref{section:libraries} identifies the most important libraries that must be supported for compatibility, and how they are supported in the proposed software platform. Section~\ref{section:expandingCompat} shows how we expanded the platform support using open source tooling. The aspects related to directly supporting CUDA programs instead of converting them first to HIP are discussed in Section~\ref{section:directCUDA}. Debugging and profiling support is outlined in \ref{section:debuggingAndProfiling}. Next we describe how the software platform portability was tested (Section~\ref{section:platformPortability}), source code portability validated (Section~\ref{section:applications}) and performance evaluated (Section~\ref{section:applications}). Finally, conclusions and future work plans are presented in~\ref{section:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{section:relatedWork}

\textit{Hipstar} is based on HIPCL~\cite{HIPCL}. Thus, HIPCL is naturally its closest relative in terms of technical aspects. \textit{Hipstar} is a result of an almost a complete rewrite of the HIPCL code base and maturing it over approximately three years of work while implementing and testing various missing essential features such as warp level primitives. This article is an greatly expands upon the original poster abstract that originally introduced the early-prototype-stage HIPCL. 

Before \textit{hipstar}, there was also an experimental conversion of the HIPCL code base to utilize Intel's Level Zero~\cite{LevelZero} low level API called HIPLZ~\cite{HIPLZ} directly. Both HIPCL and HIPLZ were merged to the same code base now called \textit{hipstar} with the direct Level Zero access provided as an additional backend for comparison purposes. Currently the recommended path from CUDA/HIP to Level Zero goes through OpenCL and PoCL's~\cite{PoCL} Level Zero driver.

We divide the other related work discussion to the following three areas: 1) Software platforms which aim to extend the portability of CUDA and/or HIP applications. The essential components in these projects are 2) the internal device program representation that provides a virtual instruction-set abstraction to cover a set of real instruction-sets, in order to enable portability of the kernel programs to various heterogeneous devices, and, 3) portability layer APIs, that hide the runtime aspects of different systems to an abstraction layer. We discuss the relevant work within these aspects in the following subsections.


%MCUDA
%– broaden the applicability of a previously accelerator-specific
%programming model to a CPU architecture
%• Swan
%– a high-level library for an application to call Swan API which is
%then mapped to the CUDA or OpenCL API
%• Coriander
%– a compiler and runtime for running CUDA applications on
%OpenCL 1.2 devices
%
%• CU2CL
%– a source-to-source translator built upon the Clang compiler for
%converting a CUDA program to an OpenCL program

\subsection{Software Platforms for CUDA Support}

\pj{TODO: ZLUDA} 
\pj{TODO: SYCLomatic} 
\pj{TODO: HIPCL}
\pj{TODO: HIPLZ}
\pj{TODO: CUDA, ROCm}
\pj{TODO: MCUDA}
\pj{TODO: Swan}
\pj{TODO: Coriander}
\pj{TODO: CU2CL}

\subsection{Device Program Representations} 

... \pj{TODO: 
Compare to the IWOCL 2023 paper which proposed LLVM IR as the IR.}
\pj{TODO: Compare to Java bytecode.}\pj{TODO: SPIR 1.2}

\subsection{Portability Layers} 

... \pj{TODO: SYCL.} 
\pj{TODO: LevelZero.} 
\pj{TODO: What other options are there?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Runtime}
\label{section:runtime}

\pj{TODO: aspects related to runtime, how to make it portable, mapping to OpenCL}

Related to memory management, a requirement \textit{hipstar} places for the supported OpenCL platforms is coarse grained Shared Virtual Memory: Unlike OpenCL, CUDA and HIP support device pointers embedded into other data structures in host memory. Implementation of this feature requires either tracking pointers and updating them before sending data to GPU, which is complex, error-prone, and in the general case impossible to perform at compile time in a watertight manner. Thus, \textit{hipstar} requires a shared virtual address space across the host and the device(s).\pj{TODO: OpenCL 3.0 requires coarse grained SVM of the minimal implementation, should we state it here clearly?}

\pj{Michal TODO (?): Describe how memory management up to certain CUDA versions works with Coarse Grained SVM only. Discuss which SVM versions or USM versions would be required for the higher levels.}

%TO SAVE SPACE:
%Additionally, the CUDA/HIP event API does not directly map to the OpenCL event API, since in CUDA/HIP the user is responsible for creating and recording events, while in OpenCL the implementation creates and records events when queuing commands.


\subsection{Lower Layer Interoperability}

\pj{TODO Michal, Brice (?): Describe the HIP-OpenCL and HIP-SYCL interoperability APIs and their use cases.}


\subsection{OpenCL-CUDA/HIP Compatibility Gaps}

\pj{Pekka TODO: This is a verbatim copy from HIPCL, to update:}
Most of the HIP API maps trivially to the OpenCL API, with some notable exceptions which might call for new OpenCL extensions:

\begin{itemize}

\item {hipGetDeviceProperties()}: for certain device properties, there is no portable way to get the information via the OpenCL device query API.

\item {hipSetDeviceFlags()}: the flags to this call control how the host thread interacts with the driver thread while waiting for the device (yield the host thread to OS, or spin wait). hipEventCreateWithFlags() provides per-event control of the synchronize behaviour (yield thread/spin wait). However, these APIs affect only performance, not correctness, thus can be implemented as no-operations.

\item {hipModuleLaunchKernel()}: passing args by ``extra'' parameter requires an API for setting all kernel arguments at once.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Device Program Compilation}
\label{section:compilation}

% TODO: points related to compilation, how we make it portable and what type
% of LLVM transformations are needed

\subsection{Program Intermediate Representation}

A major design decision for \textit{hipstar} was to choose a \textit{portable} kernel program \textit{Intermediate Representation (IR)} format that is supported by multiple OpenCL implementations and that also has solid open source infrastructure available. While the main program of a heterogeneous application is typically compiled to a native instruction-set binary of a selected host CPU, using an IR target and ``online compilation'' for the device programs enables portability of the application binary across a diversity of co-processors. 

Overall, SPIR-V seemed the best choice for the \textit{hipstar}’s ``fat binary'' format’s IR. It is used both in later OpenCL standard as well as Vulkan. It also has LLVM-based conversion tools available as open source. Therefore, SPIR-V support was set early on as a requirement for hipstar supported OpenCL targets.  Its support is not widespread in OpenCL implementations at the time of this writing, but hopefully will be as the OpenCL implementations and input producers such as \textit{hipstar} mature. Notably, neither AMD's nor NVidia's OpenCL implementations support SPIR-V at the time of this writing, but this should not be a significant problem since both platforms are directly supported by the original HIP toolset anyhow.

\subsection{Clang/LLVM-based Compilation Flow}

\pj{TODO Henry (?): draw a picture of the Clang/LLVM-based compilation chain.}
\pj{Identify the hipstar-specific LLVM passes (yet to upstream).}
\pj{TODO: Discuss the eager compilation slowness problem and how it was solved in hipstar and PoCL-level0.}

\subsection{Warp-Level Primitives}

\pj{Pekka TODO: Discuss the subgroups vs. warp issues. Lane ID.}
\pj{Pekka TODO: Non-uniform primitives.}

\subsection{Kernel Library}

\pj{Pekka TODO: This is a verbatim copy from HIPCL, to update:}
The \textit{hipstar} kernel library is an implementation of the HIP math API, by using the OpenCL C math builtins, where possible.
Many of the interfaces in the math API have an equivalent OpenCL builtin. Unfortunately, there are some functions which don't map directly, and thus require software based emulation:

\begin{itemize}

\item {Shuffle (e.g. \textit{\_\_shfl\_up()})}: While OpenCL 2.0 exposes some subgroup functions, they are not abstract enough to implement the \textit{\_\_shfl\_*} class of instructions. Intel provides an OpenCL vendor extension which exposes the shuffle capabilities of their hardware, but the semantics is slightly different from the CUDA/HIP shuffle semantics.

\item {Hardware intrinsics (e.g. \textit{\_\_fmul\_ru()})}: Many of these intrinsics implement one of the basic arithmetic operations 
% TO SAVE SPACE
%(add, sub, div, mul, and sqrt) 
with a specific rounding mode only valid for this operation. OpenCL used to support setting the rounding mode in version 1.0, but this feature has been deprecated.
% TO SAVE SPACE
%, and to our knowledge no vendor implements this feature as an extension. Therefore, this can only be implemented via software emulation or with an OpenCL vendor extension.

\item {Atomics on floating-points}: Do not exist in OpenCL,
% SAVE SPACE:
%, though they can be implemented e.g. with a CAS loop, with a performance penalty.
\end{itemize}

\subsection{OpenCL and SPIR-V Extensions}

\textit{Hipstar} compilation flow is built in a way that different OpenCL features and extensions are not required from the target platform's driver unless the input HIP application specifically needs them. In Table~\ref{table:extensions} we summarize the various extensions \textit{hipstar} can use and which CUDA/HIP feature triggers their need~\footnote{We will update the status for the final publication.}. The extensions are in different stages in the Khronos Group standardization pipeline, which is also noted in the table. 

\begin{table*}[ht]
    \centering

    \begin{tabular}{|p{6 cm}|p{6cm}|p{6cm}|}
    \hline
\textbf{Extension name} & \textbf{CUDA/HIP feature(s)} & \textbf{Status} \\
    \hline
cl\_ext\_cuda\_prec\_intrinsics     & Currently not used by \textit{hipstar}. Proposoed for the future to implement math intrinsics with reduced precision requirements which do not map to the relaxed precision limits of OpenCL, thus require an extension. & To be proposed. \\
    \hline
cl\_ext\_device\_side\_abort        & Currently not used by \textit{hipstar}. Proposed for the future to implement \_\_trap() on the low-level runtime side. & Public, potential adopters discussion. \\
    \hline                          
cl\_ext\_relaxed\_printf\_address\_space & Currently not used by \textit{hipstar}. Proposed for the future to unify printf() behavior with non-constant address spaces. & Public, potential adopters discussion. \\
    \hline
cl\_intel\_required\_subgroup\_size & When calling warp-level primitives that depend on the 32/64 warp size or the thread id ordering. & Promotion to a generalized 'khr' extension being discussed. \\ 
    \hline
cl\_khr\_command\_buffer            & CUDA graph implementation for optimized execution. & Public. \\
    \hline                          
cl\_khr\_fp64                       & If double precision floating point is used. & Public. \\
    \hline 
cl\_khr\_global\_int32\_base\_atomics \newline
cl\_khr\_global\_int32\_extended\_atomics \newline
cl\_khr\_local\_int32\_base\_atomics  \newline
cl\_khr\_local\_int32\_extended\_atomics \newline
cl\_khr\_int64\_base\_atomics \newline
cl\_khr\_int64\_extended\_atomics & Atomic operations. & Public. \\
    \hline
cl\_khr\_subgroups                  & Warp-level synchronization with \_\_syncwarp(). & Public. \\
    \hline
cl\_khr\_subgroup\_ballot           & Warp-level ballot operations. & Public. \\
    \hline
cl\_khr\_subgroup\_shuffle          & Warp-level shuffle operations. & Public. \\
    \hline
    
    \end{tabular}
    \caption{OpenCL 3.0 and SPIR-V standard extensions that \textit{hipstar} might use to implement certain CUDA/HIP features. Status is the state of the extension at the time of this article's publication.\pj{TODO: double check that we didn't forget any extension.}}
    \label{table:extensions}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Libraries}
\label{section:libraries}

The CUDA software platform and as implication, ROCm, include a set of useful libraries in addition to the general purpose program input. These libraries include common routines such as BLAS (Basic Linear Algebra Subprograms) and Deep Neural Network (DNN) acceleration libraries.

In order to support nearly drop-in compile-time compatibility with various applications that utilize either the CUDA or HIP libraries, we have ported a selection of them in a fashion that they can utilize interoperate with the \textit{hipstar} platform through interoperability interfaces. We discuss the libraries in the following and highlight their essential technical aspects.

\subsection{cuBLAS / hipBLAS}

\pj{TODO Peng, Sarbojit (?)}

\subsection{cuDNN / MIOpen}

\pj{TODO: SYCL-DNN. Involve Codeplay with this paper?}

\subsection{rocPRIM for CUB compatibility}

\pj{TODO: test with cub}

\subsection{CUDA Graphs / MIGraphX}

\pj{TODO: mapping the Graph API to the command buffer API}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Expanding Portability with Open Source OpenCL Implementations}
\label{section:expandingCompat}

Although OpenCL has again risen in popularity in the recent years, thanks to its more easily implementable OpenCL 3.0 version, the main portability gap is SPIR-V input, which is not mandated by the specification. As of this writing, only ARM and Intel support SPIR-V input to their GPU offerings in their proprietary drivers.

Fortunately there are now various active OpenCL implementations that can be used and expanded to fill up the lack of features in the proprietary implementations until they catch up. The two most vibrant ones are Rusticl~\cite{RustiCLWeb} and Portable Computing Language (PoCL~\cite{poclIJPP}). These two projects can be utilized to extend the portability to X and Y, and interestingly, back to CUDA platforms by using the PoCL-CUDA driver.

...
\pj{Discuss how we can have end-to-end testing with PoCL and support more targets thanks to its SPIR-V input.}
...
\pj{Looping Back to NV with PoCL-CUDA and/or RustiCL}
...
\pj{TODO: A portability graph which shows the layers, required extensions, and platforms that support it. Similar to https://github.com/pocl/pocl/files/10957913/sw-stack-graph.pdf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CUDA/HIP Feature Coverage}
\label{section:directCUDA}

\pj{Discuss how easy it is to add direct CUDA support too, but highlight the compatibility gaps in the next section.}
\pj{Cite and discuss the USA highest court legal ruling related to Java API, emphasize that not a lawyer, and not a focus.}

\subsection{Remaining Compatibility Gaps}
\label{subsection:compatgaps}

\pj{Discuss features specific to CUDA, from the doc we wrote in Parmance.}

\subsection{Further OpenCL Extension Needs}

\pj{CUDA intrinsics. Their accuracy is somewhat higher than what is required by the relaxed math of OpenCL.}
\pj{Command buffer: The command buffer spec currently doesn't allow SVM copy/fill commands. It doesn't also allow host interaction such as read/write buffers or host events. This means we can only support a subset of CUDA graphs with a host-side fallback necessary.}
% TODO: the OpenCL extensions identified and proposed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Debugging and Profiling Support}
\label{section:debuggingAndProfiling}

Thanks to using the open standard OpenCL as the portability layer, various debugging and profiling tooling options are available to use with little to no additional effort. The following discusses a set of tools that were successfully tested and adopted for $hipstar$ for profiling and debugging HIP programs.

\subsection{Profiling Tools}

\subsubsection{VTune}

\pj{TODO Paulius?}

\subsubsection{Tracing Heterogeneous APIs (THAPI)}

% https://github.com/argonne-lcf/THAPI

\pj{TODO Brice?}

\subsection{Debugging}

\pj{TODO: PoCL-CPU, GDB, Valgrind, debug info...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Platform Portability Experiments}
\label{section:platformPortability}

\subsection{AMD and Intel CPUs}

\pj{Using Intel OpenCL CPU and/or PoCL-CPU}

\subsection{ARM Mali GPU}

\pj{Since ARM Mali supports SPIR-V, we could run a portability experiment with CUDA/HIP running on a smartphone. I can try to ask someone from TAU to do this. But even better, Brice, could we involve Kevin Petit to help with this? It would not be much to ask given they are getting HIP/CUDA input basically for free (well, how much benefit that has on Android platforms is questionable)...}

\subsection{RISC-V}

\pj{If we manage to do the port in time, we could show execution on  RISC-V CPUs.}

\subsection{CUDA}

\pj{Back to CUDA via PoCL-CUDA}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Application Portability Validation}
\label{section:applications}

In order to validate the drop-in compatibility level of \textit{hipstar}, we ported a set of well-known applications and libraries used widely in HPC that target either the CUDA or HIP API. The applications along with any source code modifications required are outlined in the following.

\pj{TODO: also highlight which compatibility library was tested}

\subsection{CP2K}

\pj{TODO who writes?}

\subsection{Exabio}

\pj{TODO who writes?}

\subsection{GAMESS}

\pj{TODO: Colleen? Evaluate how well it works in terms of functionality and compare to another platform in perf.}

\subsection{libCEED}

\pj{TODO: Colleen?}

\subsection{Pytorch-HIP}

\pj{TODO: Henry?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Performance Evaluation}
\label{section:performance}

In this section we evaluate the performance of \textit{hipstar}  with specific focus on overheads cause by layering HIP and CUDA on top of other API layers such as OpenCL and Level Zero.

\subsection{Execution Performance}

\pj{TODO: Which apps and which GPUs we should use here?}

\subsection{Layering Overheads}

\pj{TODO: Evaluate how much overhead on top of straight HIP/CUDA to CUDA vs. through PoCL-CUDA}
\pj{TODO: Evaluate how much overhead going through PoCL-level0 incurs on top of straight LZ BE}

\subsection{Overhead Analysis}

\pj{Straight to CUDA vs. CUDA to HIP to hipstar to OpenCL + to OpenCL to PoCL-CUDA to CUDA}


\pj{TODO: Comparisons}
\pj{TODO: Overhead analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions and Future Work}
\label{section:conclusions}

\bibliography{IEEEabrv,hipstar}
\bibliographystyle{IEEEtran}

\end{document}
