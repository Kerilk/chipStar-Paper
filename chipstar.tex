\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{color}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{svg}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{draftwatermark}
\SetWatermarkText{DRAFT}
\SetWatermarkScale{1}

% https://tex.stackexchange.com/questions/326897/vertical-alignment-of-a-turned-cell
\usepackage{rotating}
\usepackage{array,makecell,multirow}

\usepackage{ifthen}
\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
{ \newcommand{\mynote}[3]{
     \fbox{\bfseries\sffamily\scriptsize#1}
        {\small$\blacktriangleright$\textsf{\emph{\color{#3}{#2}}}$\blacktriangleleft$}}
  \newcommand{\newtext}[1]{{\color{orange}{#1}}}}
{ \newcommand{\mynote}[3]{}
  \newcommand{\newtext}[1]{#1}}

% Please use a named note with this macro to comment the text:
\newcommand{\pj}[1]{ \mynote{PJ}{#1}{blue} }
\newcommand{\bv}[1]{ \mynote{BV}{#1}{green} }
\newcommand{\mb}[1]{ \mynote{MB}{#1}{cyan} }
\newcommand{\cb}[1]{ \mynote{CB}{#1}{magenta} }
\newcommand{\pv}[1]{ \mynote{PV}{#1}{yellow} }
\newcommand{\ba}[1]{ \mynote{BA}{#1}{brown} }

\newcommand{\hiplz}{\texttt{HIPLZ}\xspace}
\newcommand{\hipcl}{\texttt{HIPCL}\xspace}
\newcommand{\hip}{\texttt{HIP}\xspace}
\newcommand{\opencl}{\texttt{OpenCL}\xspace}
\newcommand{\lz}{\texttt{L0}\xspace}
\newcommand{\sycl}{\texttt{SYCL}\xspace}
\newcommand{\cuda}{\texttt{CUDA}\xspace}
\newcommand{\chipstar}{\textit{chipStar}\xspace}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

% IEEE policy on preprints seems to be reasonable:
% https://journals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelines-and-policies/submission-and-peer-review-policies/#electronic-reprints

% https://journals.ieeeauthorcenter.ieee.org/submit-your-article-for-peer-review/the-ieee-article-submission-process/
% TPDS manuscript types and submission length guidelines are described below. All page limits include references and author biographies. For regular papers, pages in excess of these limits after final layout of the accepted manuscript is complete are subject to Mandatory Overlength Page Charges (MOPC). Note: All supplemental material must be submitted as separate files and must not be included within the same PDF file as the main paper submission. There is no page limit on supplemental files. 

% Regular paper – 12 double column pages (Submissions may be up to 18 pages in length, subject to MOPC. All regular paper page limits include references and author biographies.)

\begin{document}

\title{chipStar: A Compiler and Runtime for Cross-Vendor Portable HIP/CUDA Based on Open Standards}

%\author{pekka.jaaskelainen }
%\date{March 2023}

\author{Pekka Jääskeläinen, Henry Linjamäki, Michal Babej, Peng Tu, Sarkar Sarbojit, Ben Ashbaugh, Colleen Bertoni, Kevin Harms, Paulius Velesko, Philip C. Roth, Rahulkumar Gaytri, Jisheng Zhao, Brice Videau
        % <-this % stops a space
\thanks{Pekka Jääskeläinen, Henry Linjamäki, Michal Babej, Peng Tu, Sarbojit Sarkar and Ben Ashbaugh(?) are with Intel Corporation. \textit{Corresponding author: Pekka Jääskeläinen, email: pekka.jaaskelainen@intel.com}.}
\thanks{Pekka Jääskeläinen is also with Tampere University, Finland. }
\thanks{Paulius Velesko is with Pagan LC.}
\thanks{Brice Videau, Colleen Bertoni and Kevin Harms are with Argonne National Laboratory, ...}
\thanks{Philip C. Roth is with Oak Ridge National Laboratory, ... }
\thanks{Rahulkumar Gaytri is with National Energy Research Scientific Computing Center, ...}
\thanks{Jisheng Zhao is with Georgia Institute of Technology, Atlanta, Georgia.}
\thanks{\pj{The authors are not in any particular order. I put myself as 1st author as I'm leading the writing, and then I ordered the co-authors according to their affil.}}
%\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{IEEE Transactions on Parallel and Distributed Systems,~Vol.~X, No.~Y, Month~YEAR}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

%This document describes the most common article elements and how to use the IEEEtran class with \LaTeX \ to produce files that are suitable for submission to the IEEE.  IEEEtran can produce conference, journal, and technical note (correspondence) papers with a suitable choice of class options.

%\pj{This first paragraph is optional, we can remove it:}
%Due to NVIDIA dominating the GPU market and despite its lack of cross-vendor portability, the C/C++-based application programming interface of CUDA and its related key libraries are still used in a significant fraction of software utilizing GPU-based acceleration. AMD's ROCm and its Heterogeneous-compute Interface for Portability (HIP) aims to alleviate the CUDA's lack of portability by providing a route out from the NVIDIA CUDA platform to AMD's devices.

In this article we describe \textit{chipstar}, an open source software tool which allows running CUDA and HIP programs on an a open cross-vendor standard software platform consisting of an OpenCL/SPIR-V backend and a carefully crafted set of standard extensions. We present the relevant technical aspects of \textit{chipstar} related to the feature mismatches between CUDA/HIP and OpenCL and exemplify its runtime overheads in comparison to executing CUDA/HIP applications directly with NVIDIA's CUDA software platform. The measurements show that the overheads induced by \textit{chipstar} are typically only in the order of N-M\%, thus negligible.\pj{TODO} Although being a relatively young code base, \textit{chipstar} is now considered mature enough for wider testing and even production use, which is demonstrated by the described application porting case studies deployed for the Aurora supercomputer.

\end{abstract}

\begin{IEEEkeywords}
CUDA, HIP, OpenCL, SPIR-V, Portability, Shared Virtual Memory
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\IEEEPARstart{W}{alled} garden strategy is popular within market dominating companies. Its purpose is to lock-in customers to company's products by making escaping the gates of the garden as costly as possible. NVIDIA's CUDA software platform is considered to be one of such walled gardens. It in part helps NVIDIA to expand and keep a foothold of their GPU market advantage, and at the same time maintain high innovation pace on the software APIs since there is no need to work with standardization committees that always have to aim for a consensus among multiple participating vendors.

For the end-users and other hardware vendors, naturally, the situation of a single-vendor dominating API is not optimal. End-users benefit from open standard software interfaces that enable switching the hardware without incurring significant non-recurring engineering costs required for porting all the legacy applications and libraries to a new software platform just to be able to utilize the purchased hardware optimally. Similarly, other hardware vendors, aiming to get their piece of the market pie, would prefer an API that is not dictated by a single vendor.

AMD's ROCm~\cite{ROCm} software platform and its Heterogeneous-compute Interface for Portability (HIP) language~\cite{hip} helps escaping the CUDA walled garden by providing a route out from the NVIDIA CUDA platform to AMD's devices. HIP defines a subset of CUDA that is more easily portable to various hardware, thanks mainly to omitting various advanced features available in the later CUDA versions. 
%(some of these features are discussed in Section~\ref{subsection:compatgaps}). 
In order to enable easy automated path from CUDA applications, HIP is largely a copy of a CUDA C/C++ subset with a few minor differences, and renamed function names. It alleviates the CUDA portability problem, but doesn't solve it satisfactorily due to AMD targeting primarily their self-specified low level ROCm APIs which have not been so far ported to other than AMD platforms. 
%An open source CUDA/HIP software platform solely based on open standards with a sincere aim for cross-vendor portability is still lacking.

In this article we make the following contributions:

\begin{enumerate}
  \item We publish internal design choices of a complete open source software platform that enables porting applications from NVIDIA-driven CUDA and AMD-driven ROCm platforms to any platform supporting the cross-vendor open standards OpenCL and SPIR-V,
  \item evaluate the portability aspects on platforms with proprietary drivers,
  \item extend an open source OpenCL implementation to expand the portability on more device types, and
  \item provide an extensive related work review for contemporary API options for implementing the portability layer for heterogeneous programming models.
\end{enumerate}

In comparison to the previous solutions, our proposed solution typically enables source-modification-free compilation of HIP/CUDA programs to a runtime portable ```fat binary'' that utilizes solely open standards.
%with supporting the necessary library dependencies for the most essential CUDA/ROCm APIs.

The rest of the article is organized as follows: \pj{To update:} Section~\ref{section:runtime} discusses the technical challenges related to the runtime execution. Section~\ref{section:compilation} describes the most relevant parts of the compiler support. Section~\ref{section:libraries} identifies the most important libraries that must be supported for compatibility, and how they are supported in the proposed software platform. Section~\ref{section:expandingCompat} shows how we expanded the platform support using open source tooling. The aspects related to directly supporting CUDA programs instead of converting them first to HIP are discussed in Section~\ref{section:directCUDA}. Debugging and profiling support is outlined in Section~\ref{section:debuggingAndProfiling}. Then we describe how the software platform portability was tested (Section~\ref{section:platformPortability}), source code portability validated (Section~\ref{section:applications}) and performance evaluated (Section~\ref{section:applications}). Section~\ref{section:relatedWork} overviews the related work to the proposed software platform and its components. 
Finally, conclusions and future work plans are presented in~\ref{section:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Runtime}
\label{section:runtime}

The primary target for \chipstar is to support the subset of CUDA features
defined by HIP and expanding the feature set beyond it whenever feasible.
At the time of this writing, HIP refers to features only at CUDA version 9.0
or older

%, thus excludes modern functionality available in later NVIDIA
%devices such as page-fault relying unified memory.

CUDA and our chosen portability layer API, OpenCL, share various platform and
memory model abstractions, for example, ``device memory'' is the same as
``global memory'' in OpenCL terminology (``shared'' is ``local'').
To avoid confusion in terminology we utilize only the CUDA terms in the rest of
this article. Similarly, we refer to the original CUDA versions when talking about
functions that have been cloned in the HIP API.

\subsection{Memory Management}

The main interface in CUDA's device memory management is \textbf{cudaMalloc()}. It returns a
raw pointer to the targeted device's global memory, instead of an opaque handle as is the
case with OpenCL's basic buffer management functionality. This presents a divergence
from the basic OpenCL specification for device memory management, which only provides a buffer
management API (\textbf{clCreateBuffer()} and others) which returns opaque handles (cl\_mem).

\pj{About CUDA's "implicit address space" vs. generic of OpenCL/SPIR-V?}

The opaque buffer handles cannot be used to implement CUDA device memory management
because it is not possible perform pointer arithmetics or pass addresses in other data structures,
which is allowed by CUDA. In order to implement these capabilities, we utilized the
Shared Virtual Memory (SVM) API of the OpenCL specification to allocate device pointers.
The SVM API allocates memory and returns a raw pointer to a shared virtual address space
region. SVM's ``Coarse Grained buffer'' variant
suffices to implement the basic device memory management features of CUDA. However, using SVM for
implementing device memory management has a drawback that the device must support SVM (map
the allocated regions to the same addresses in the host point of view) although just providing accesses physical device
pointers would suffice. This means that the \chipstar implementation is actually
implementing CUDA's Unified Memory model by default.%~\pj{TODO: Utilize USM device-side allocations to optimize the device-only allocations?}.

In OpenCL coarse grained SVM, memory consistency between device and host memories is guaranteed at
the execution boundaries of kernel commands referring to the SVM allocation. The kernels
referring to the SVM allocations must either use the SVM allocations as argument buffers, or
be explicitly denoted to refer to allocations used indirectly. The latter poses a challenge
since in principle any kernel can refer to any previous allocation as CUDA device pointers can
be passed inside data structures or global variables. For coarse grained SVM,
OpenCL's \textbf{clSetKernelExecInfo()} must be used to list all potentially used SVM allocations.
This poses a possible performance overhead since the \chipstar runtime must play it safe
and register all possible previous allocations to any kernel, unless proven that the
kernel doesn't refer to a particular allocation. It can then lead to unnecessary data
synchronizations between the host and the device memories.
%\pj{TODO: optimize via a simple kernel analysis.}

CUDA provides an API to \textit{pin} memory so its kept resident in the device memory and
accessible directly by the host, and is not swapped out to disk. The primary APIs to this functionality are
\textbf{cudaHostAlloc()} and \textbf{cudaHostRegister()}. The former allocates pinned
memory directly and the latter pins a previous device allocation. \textbf{cudaHostAlloc()}
is simple to implement with coarse grained SVM since, by coincidence of using
a shared virtual memory allocation, the buffers are available to both the host and
the device. The allocation might not be pinned for the duration of the execution if
the implementation can swap such allocations safely, but that aspect is only potentially
inspectable in performance difference. However, \textbf{cudaHostRegister()} is
more challenging to implement on top of SVM since it allows registering a host address
range to be a pinned region accessible both from the host and the device \textit{after}
the host memory has been allocated. Since the host memory might not been allocated with the
OpenCL SVM API, but with a system memory allocator or even from the stack, to implement
correct functionality \chipstar has to create a shadow buffer using \textbf{clSVMAlloc()} and
synchronize it with the host region at kernel start and end.\pj{TODO: can USM pin host memory afterwards?}

% https://developer.nvidia.com/blog/unified-memory-cuda-beginners/

The later NVIDIA architectures (since compute capability 6) support on-demand page migration which
relies on hardware memory management unit (page fault based buffer migrations) for coherence
on the Unified Memory allocations. This frees the programmer from the need to perform explicit memory
allocation and synchronization calls. This functionality maps to the Fine-Grained System SVM of OpenCL,
but since its support by hardware and drivers is very rare at the time of this writing, it is not
yet implemented by \chipstar.

\pj{TODO: describe how we utilize USM to optimize the memory management - as soon as the USM patch gets in.}

\pj{Indirect access optimizations. The simplest allocation method in SYCL is the buffer allocation for which there is no match in HIP/CUDA, but the simplest one is cudaMalloc() which requires pointer-based access also on the device side. To optimize unnecessary indirect access annotations, we implement...}

\subsection{Task Synchronization}

...
The CUDA/HIP event API does not directly map to the OpenCL event API, since in CUDA/HIP the user is responsible for creating and recording events, while in OpenCL the implementation creates and records events when queuing commands. Recording is implemented by creating a new Marker-type cl\_event (clEnqueueMarkerWithWaitList) when hipEventRecord is called. \pj{Michal (?) TO expand} \mb{not sure what else to write here}

\pj{TODO: Asynchronous stream execution.}

\subsection{Lower Layer API Interoperability}

\pj{TODO Sarbojit: Describe the HIP-OpenCL and HIP-SYCL interoperability APIs and their use cases.}
\pj{Can you add code examples of using the different interop APIs?}

The native interoperability API can be used to initialize HIP context (with assigned device \& command queue) from a set of native (LevelZero/OpenCL) object handles, or in the opposite direction to retrieve a set of native handles from an existing HIP context. Thread-safe use of handles is currently left to the application (which should be non-issue with OpenCL since it is thread-safe). Additionally, there are two APIs that create a HIP event from native event handle, and vice-versa. These can be used for interoperability of HIP code with native code while maintaining asynchronous execution.\pj{Can we share buffers somehow between APIs? Or is that down to the "external memory extension"?}\pj{Is the SYCL interop via LZ/OpenCL, no direct API calls?}

%\subsection{OpenCL-CUDA/HIP Compatibility Gaps}

%\pj{Pekka TODO: This is a verbatim copy from HIPCL, to update:}
%Most of the HIP API maps trivially to the OpenCL API, with some notable exceptions which might call for new OpenCL extensions:\pj{TODO: We should just make them extensions (proposals) to clean up the story.}

%\mb{Pekka TODO: do we also list APIs which can be implemented but aren't yet (because nobody's done the work) ? looking quickly at CHIPBindings.cc, there are >50 hip API functions which have not been implemented, things like Peer2peer, hipIPC*, hipModuleOccupancy*, hipProfiler*, hipMemPool*, hip{Malloc,Free}Async etc; some might require OpenCL extensions }
%\pj{I think not worth listing here, as it's only a matter of time when these are implemented and if apps do not use them, they %are not high prio.}

%\begin{itemize}

%\item {hipGetDeviceProperties()}: for certain device properties, there is no portable way to get the information via the OpenCL device query API.\pj{this should be an easy extension}

% Pekka> I think we can do without this as it's visible only in terms
% of latency/performance to the user, and there should be also other
% similar features which can be observed only in terms of perf., not
% functional correctness (e.g. the typical concurrency to parallelism mapping).
%\item {hipSetDeviceFlags()}: the flags to this call control how the host thread interacts with the driver thread while waiting for the device (yield the host thread to OS, or spin wait).
%
%\item{hipEventCreateWithFlags()}: provides per-event control of the synchronize behaviour (yield thread/spin wait). However, these APIs affect only performance, not correctness, thus can be implemented as no-operations.

% Pekka> Cannot we really implement this without an extension even if we had kernel metadata to traverse? \mb{possibly, if we can always figure out the correct alignments & padding}
%\item {hipModuleLaunchKernel()}: passing args by ``extra'' parameter requires an API for setting all kernel arguments at once.\pj{a new clEnqueueNDRange variation with a HSA-style-specified exact layout argument buffer layout might be useful in any case.}

%\item {hipGraph API}: the API to create, update & launch graphs. The existing cl_khr_command_buffer extension is not sufficient, since we need to work with SVM. (discussed below in the "opencl and spirv extensions" section).

%\item {hipHostRegister}: we'll need an OpenCL extension to implement this (unless there is something already we could use, i haven't checked).

%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Compilation}
\label{section:compilation}

% TODO: points related to compilation, how we make it portable and what type
% of LLVM transformations are needed

\subsection{Program Intermediate Representation}

A major design decision for \textit{chipstar} was to choose a \textit{portable} kernel program \textit{Intermediate Representation (IR)} format that is supported by multiple OpenCL implementations and that also has solid open source infrastructure available. While the main program of a heterogeneous application is typically compiled to a native instruction-set binary of a selected host CPU, using an IR target and ``online compilation'' for the device programs enables portability of the application binary across a diversity of co-processors. 

Overall, SPIR-V~\cite{SPIRV} seemed the best choice for the \textit{chipstar}’s ``fat binary'' format’s IR.\pj{emphasize that it's a cross-vendor committee driven standard, in good and in bad} It is supported both in later OpenCL standard as well as Vulkan. It also has good LLVM-based conversion tools available as open source. Therefore, SPIR-V support was set early on as a requirement for \textit{chipstar} supported OpenCL targets.  Unfortunately, its consumption support is not widespread in OpenCL implementations at the time of this writing, but hopefully this will change as the OpenCL implementations and input producers such as \textit{chipstar} mature. Notably, neither AMD's nor NVIDIA's OpenCL drivers support SPIR-V at the time of this writing. This should not be a significant problem in terms of HIP/CUDA portability since both platforms have direct support for them through their own implementations.

\subsection{Compilation Flows}

The offline compilation flow of \chipstar is based on the LLVM Project's~\cite{LLVM} Clang~\cite{Clang} frontend. The compiliation process is shown in Fig.\ref{fig:compilation}. It relies on the CUDA/HIP frontend of Clang, which was extended to produce SPIR-V binaries instead of PTX or AMDIL for the device program. 

\begin{figure*}
    \centering
    \includegraphics[scale=1]{figs/chipstar-compilation-v2.pdf}
    \caption{The offline compilation flow. The offline device builtin library provides target independent HIP builtin implementations. The opt tool runs LLVM passes provided by \textit{chipStar} for lowering HIP features to OpenCL-SPIR-V environment. The SPIR-V translation is performed using Khronos' LLVM-SPIRV-Translator tool}
    \label{fig:compilation}
\end{figure*}

Most of the compilation related changes have been upstreamed to the LLVM project and very little compilation related functionality remains in the \chipstar project. The notable exceptions are: \pj{printf} \pj{abort} \pj{globals} \pj{what else} \pj{maybe we should implement the divergent/exited-thread barrier detection based on divergence analysis?}

\pj{Mention what was upstreamed to the LLVM Project to make chipstar work.}
\pj{Identify the chipstar-specific LLVM passes yet to upstream.}

\pj{Henry (?) TODO: Discuss the eager compilation slowness problem and how it was solved in chipstar and PoCL-level0.}

Fig.\ref{fig:online-compilation} shows online compilation flow from SPIR-V to device code in \textit{chipStar} runtime. A SPIR-V module is compiled just-in-time when a kernel associated with it is launched. The online device builtin library provides HIP builtin functions based on device capabilities which are linked to the user’s device programs. For example, for HIP floating-point atomics the runtime chooses between implementation that maps them to corresponding native function or emulates them via atomic exchange operations.

\begin{figure*}
    \centering
    \includegraphics[scale=1.0]{figs/chipstar-rt-compile-n-link.pdf}
    \caption{The just-in-time compilation flow in \chipstar}
    \label{fig:online-compilation}
\end{figure*}

\subsection{Handling Warp-Level Primitives}

CUDA and HIP platforms have a finer grained grouping of the threads (OpenCL work-items) executing the blocks (work-groups) called a warp. In the earlier CUDA versions \pj{clarify}, the threads in a warp could be assumed to execute in lock-step, implying that the enabled threads in the same warp would execute the same instruction. This implied that in some cases explicit synchronization could be omitted: In case of a usual read-modify-update case, the programmer could trust that the warp's threads all execute the read part before any of them proceed to the update part, enabling in-place-updates without explicit synchronization. However, with the later\pj{specify} releases of CUDA relying on lock-step behavior was deprecated~\cite{NVIDIAProgrammersManual}. ...

However, the fixed size warps (32 threads for NVIDIA and usually 64 threads in AMD devices) affect the execution semantics when executing warp-level primitives that rely on the warp grouping and the mapping of the threads to the lanes of the warp.  Such primitives include the warp shuffles, which read data from a specific lane within the warp, and the explicit warp synchronization primitives. 

\pj{There could be a figure here with possible subgroup id mappings and how warps always map the threads in linear order.}
The base OpenCL specification, on the other hand, doesn't have a warp concept, but the work-items are free to make progress in any order and grouping. It has an extension called ``subgroups'' which can be used to implement the warp semantics when the kernel needs them. In contrast to  warps which have a specified form and content which allows the programmer to utilize them reliably, the basic subgroups of OpenCL are ``implementation-oriented''; they enable grouped execution in a manner that the is simplest or most efficient for the driver: The sizes of the subgroups are not fixed, but must be queried per kernel by the programmer in the basic extension. Also the way work-items are mapped to subgroup lanes (so they can be referred to when using cross-lane intrinsics) is also implementation-defined. To close the gap between subgroups and warps, an incremental specification extension \textit{cl\_intel\_reqd\_subgroup\_size} that \textit{forces} the subgroup size of the kernel to the desired size along with the linear id mapping was proposed.

% HIP doesn't support the new _sync-ones, so let's focus on it.
% Maybe also in the title of the paper.
%\pj{Pekka TODO: Non-uniform primitives.}

\subsection{Device Built-in Library}

The \textit{chipstar} device-side library implements the HIP math API, by using a combination of OpenCL C math builtins, OCML (part of ROCm-Device-Libs), and custom implementations.
A lot of the functions in the math API have an equivalent OpenCL builtin with adequate accuracy guarantees with a few exceptions that cannot be mapped directly, and thus require software based emulation such as floating-point atomics on some devices. The main challenge in terms of a fast yet portable implementation of the functions are due to differences in math accuracy requirements between CUDA/HIP and OpenCL: most of the standard math functions of CUDA are defined in higher accuracy than what the OpenCL standard requires. 

Furthermore, CUDA/HIP defines a set of \textit{intrinsics}, which are faster yet less accurate versions of the standard functions. This exposes a further difficulty when aiming for a portable yet fast implementation: It heavily depends on the targeted platform what level of accuracy is achievable while still enabling execution time benefits. Since CUDA is inherently meant not to be cross-vendor portable, the intrinsics are defined only to match the CUDA microarchitecture in an optimal manner, which might not be the case for other devices. 

OpenCL covers the use case of accessing fast but inaccurate hardware operations by means of a relaxed mathematics flag that can be enabled at device program build time and with so called native built-in functions in the built-in kernel API. Unfortunately, neither of these are usable for implementing the CUDA intrinsics by default due to not guaranteeing enough accuracy: The relaxed math in OpenCL defines maximum rounding errors, but they are usually slightly less than what the CUDA intrinsics require. The OpenCL native built-in functions are even worse fit for this use since they guarantee nothing of the accuracy, but leave it entirely up to the implementation. There is not even a possibility to query for the maximum error via a runtime API, but the accuracy must be discovered via trial-and-error or from documentation of the hardware vendor.\pj{TODO: Check the HIP statements of guaranteed accuracy.}

The ``correctness first'' principle requires implementing the functionality by default with guaranteed accurate enough arithmetics, which means to not receive any performance benefits of simplified implementations. 

Correctness for the basic math functions would require software emulating them with added accuracy, of which performance impact would likely be too drastic to make \chipstar unusable for high performance workloads for which it is typically used. We chose a middle-ground where the basic math functions are implemented at the OpenCL accuracy by default and the intrinsics also utilize the default functions instead of the native functions with unspecified accuracy (thus do not get any performance benefits). For the workloads we tested, this seemed to be a good enough solution.  

We plan to improve this aspect in the future via a new standard extension with a set of builtins that guarantee the CUDA accuracy requirements to the application programmer while enabling the targeted platform to optimize and implement them as efficiently as possible.




% https://github.com/CHIP-SPV/chip-spv/issues/222
% https://intel.github.io/llvm-docs/cuda/cuda-vs-opencl-math-builtin-precisions.html

\pj{TODO Henry(?): About the textures implementation. It might need some explanation in the compiler part too?}

\subsection{OpenCL and SPIR-V Extensions}

\textit{chipstar} compilation flow is built in a way that different advanced OpenCL features and extensions are not required from the target platform's driver unless the compiled input application specifically needs them. 
Although the minimal OpenCL 3.0 feature set with coarse-grained SVM and SPIR-V consumption support covers a significant part of CUDA and HIP features, some features require or can be improved with various extensions to the OpenCL or SPIR-V specifications. 

In Table~\ref{table:extensions} we summarize the standard extensions \textit{chipstar} can utilize and which CUDA/HIP feature triggers their need. The extensions are in different stages in the Khronos Group standardization process, which is also noted in the table.~\footnote{Note to reviewers: We will update the status for the final article version.} 

\begin{table*}[ht]
    \centering

    \begin{tabular}{|p{6 cm}|p{6cm}|p{6cm}|}
    \hline
\textbf{Extension name} & \textbf{CUDA/HIP feature(s)} & \textbf{Status} \\
    \hline
cl\_ext\_alive\_only\_barrier       & A special work-group barrier for barrier calls which might not be reached by work-items that have exited the kernel. & To be proposed. \\
    \hline
cl\_ext\_cuda\_math     & Implement math functions and intrinsics with precision requirements that match CUDA's. & To be proposed.  \\
    \hline
cl\_ext\_device\_side\_abort        & Proposed for the future to implement \_\_trap() on the low-level runtime side. & Public.  \\
    \hline 
cl\_ext\_extended\_device\_properties & hipGetDeviceProperties() can be used to query more device properties than the basic OpenCL device or platform query APIs support, this extends the spec to fill the gap. & To be proposed. \\
    \hline                          
cl\_ext\_relaxed\_printf\_address\_space &  Proposed for the future to unify printf() behavior with non-constant address spaces. & Public. \\
    \hline
cl\_intel\_required\_subgroup\_size & When calling warp-level primitives that depend on the fixed warp size or the thread id ordering. & Public. Promotion to a general 'khr' extension proposed. \\ 
    \hline
cl\_intel\_unified\_shared\_memory & Used for optimized memory management, when available.  & Public. To promote to a general 'khr' extension. \\
   \hline
cl\_khr\_command\_buffer            & Optimized implementation of CUDA graph execution. & Public. SVM command extension proposed.  \\
    \hline 
cl\_ext\_command\_buffer\_host\_data & CUDA graphs which transfer data between the host and the device. & To be proposed. \\
    \hline 
cl\_ext\_command\_buffer\_host\_sync & CUDA graphs which synchronize with the host. & To be proposed. \\

    \hline                          
cl\_khr\_fp64                       & If double precision floating point is used. & Public. \\
    \hline 
cl\_khr\_global\_int32\_base\_atomics \newline
cl\_khr\_global\_int32\_extended\_atomics \newline
cl\_khr\_local\_int32\_base\_atomics  \newline
cl\_khr\_local\_int32\_extended\_atomics \newline
cl\_khr\_int64\_base\_atomics \newline
cl\_khr\_int64\_extended\_atomics & Atomic operations. & Public. \\
    \hline
cl\_khr\_subgroups                  & Warp-level synchronization with \_\_syncwarp(). & Public. \\
    \hline
cl\_khr\_subgroup\_ballot           & Warp-level ballot operations. & Public. \\
    \hline
cl\_khr\_subgroup\_shuffle          & Warp-level shuffle operations. & Public. \\
    \hline
    
    \end{tabular}
    \caption{OpenCL 3.0 standard extensions that \textit{chipstar} might use to implement CUDA/HIP features. Status describes the state of the extension at the time of this article's publication. The number of vendor-specific extensions is low and expected to shrink in the future.}
    \label{table:extensions}
\end{table*}

Most of the extensions are straightforward and the explanation in the table cell suffices to grasp their purpose. However, X and Y require further discussion: \pj{...}
\pj{Note that the warp-primitives section already discussed reqd SG size}

\subsection{Compiling CUDA Applications Directly}
\label{section:directCUDA}

While the primary goal of \chipstar is to cover the CUDA/HIP APIs to the extent defined by the HIP programmer's manual, \chipstar implementation also supports a set of CUDA APIs directly. The ability to call CUDA APIs directly drops the need for source-to-source translations when porting originally CUDA applications to the platform. This is done by simply delegating the CUDA API calls to the HIP versions, similar to what HIP does with their CUDA mapping, but in reverse.\pj{to check}

There has been legal controversy related to APIs how they are covered by the copyright laws in the past that has made legality of direct implementations of proprietary APIs unclear. This changed with the Supreme Court of the United States ruling of April 5, 2021 in the Google LLC vs. Oracle America, Inc. case, which stated that copying the Java API for use in the Android OS was considered ``fair use'' since it was done for compatibility purposes:

\begin{quote}
``Google’s copying of the Java SE API, which included only those
lines of code that were needed to allow programmers to put their accrued talents to work in a new and transformative program, was a fair
use of that material as a matter of law.''~\cite{JavaSupreme}
\end{quote}

Although no code was copied directly from the NVIDIA implementation, even if it was the case we believe
our limited implementation of the CUDA API falls well within such fair use outlined in the ruling. However, since we, the \chipstar developers are engineers, not lawyers, we wanted to be extra careful that copyrights were not disrespected in any jurisdiction when adding support for direct CUDA API calls by using an implementation approach where only the programmer's manual was consulted for the API reference when implementing the CUDA compatibility headers to the \chipstar code base. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Libraries}
\label{section:libraries}

The CUDA software platform and as implication, ROCm, include a set of useful libraries in addition to the general purpose program input. These libraries include common routines such as BLAS (Basic Linear Algebra Subprograms) and Deep Neural Network (DNN) acceleration libraries.

In order to support nearly drop-in compile-time compatibility with various applications that utilize either the CUDA or HIP libraries, we have ported a selection of them in a fashion that they can utilize and interoperate with the \textit{chipstar} platform. We discuss the libraries in the following and highlight their essential technical aspects.

\subsection{cuBLAS / hipBLAS}

\pj{This looks good. Let's add a short intro that the library is not portable (we need to explain this since this article's fundamental promise is enhancing portability of CUDA/HIP to various platforms), but an example that we can interface at the API level with as efficient as possible target-specific libraries implemented behind it. And that it can interface efficiently with the CHIP-SPV's relevant structures such as streams.}

We implemented a oneMKL backend to the hipBLAS performance library to run on Intel GPUs. Applications calling hipBLAS functions can run without any code changes.
The oneMKL backend for hipBLAS uses CHIP-SPV interoperability with SYCL to invoke the oneMKL’s SYCL functions. During the hipBLAS handle creation, the oneMKL backend extracts the native queue handle of a HIP stream using interop APIs from CHIP-SPV.  It uses the native queue handle to create a corresponding SYCL queue to execute the  oneMKL functions for the calls initiated from the hipBLASS handle. Since SYCL and CHIP-SPV both support level-zero and opencl runtimes, the oneMKL backend for hipBLAS also supports both. Users can switch between them using environment variables and the rest will work transparently in the hipBLAS library.

The hipBLAS is a Intel only library where hipBLAS interfaces can run on Intel GPU. In below study we have measured single precision GEMM function performance with 2048 x 2048 matrix size comparing hipBLAS, oneMKL SYCL and oneMKL OpenMP runtime and APIs. We used a pre-production Intel PVC system \pj{this might be a problem due to NDA?} and measured executions on 1-tile.  Fig.~\ref{fig:hipBlas-rel-perf} shows the relative performance with oneMKL SYCL as the base.  We found that the performances are closely matched across all three runtimes.\pj{Not sure if it's worth adding the different iteration counts. If you look at the other results later, we took the best execution times over 100 iterations to give the roofline for perf. achievable. The perf. should vary only with a cold cache or if there's some other load in the system competing for the resources, both cases of which we should filter out this way.}


Currently, oneMKL does not provide a pointer-mode API as defined in HIP and CUDA. We have added a wrapper in oneMKL backend for hipBLAS to emulate pointer mode under oneMKL. In HIP device pointer mode, the oneMKL backend needs to copy scalar parameters, such as ‘alpha’ and ‘beta’ in GEMM between host and device memories.  Fig.~\ref{fig:hipBlas-host-vs-dev} shows more than 50\% performance drop with the workaround hence it is advised to avoid using device pointer mode in the current release.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering             
         \includegraphics[width=1\textwidth]{figs/comparision_between_sycl_hip_openmp.pdf}
         \caption{Average time taken for 5k gemm run with different runtime. Data is normalized with Sycl as 1.}
         \label{fig:hipBlas-rel-perf}
     \end{subfigure}     
     \begin{subfigure}[b]{0.5\textwidth}
         \centering             
         \includegraphics[width=1\textwidth]{figs/comparision between_host_and_dev_ptr.pdf}
         \caption{Average time taken for 5k gemm run with host and device pointers. Data is normalized with host pointer as 1.}
         \label{fig:hipBlas-host-vs-dev}
     \end{subfigure}
     \caption{...\pj{todo}}
\end{figure}

\subsection{cuDNN / MIOpen}

\pj{TODO: SYCL-DNN. Involve Codeplay with this paper?}

\subsection{rocPRIM for CUB compatibility}

\pj{TODO: test with cub}

\subsection{CUDA Graphs / MIGraphX}

\pj{TODO Michal: Describe mapping the Graph API to the command buffer API}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Expanding Portability Using Open Source OpenCL Implementations}
\label{section:expandingCompat}

Although OpenCL has again risen in popularity in the recent years, thanks to its more easily implementable OpenCL 3.0 version, the main optional features hindering portability of \chipstar are SPIR-V input and coarse-grain SVM. As of this writing, only ARM GPUs and Intel GPUs and CPUs support SPIR-V input in the vendor-supported drivers.

Fortunately there are now various active OpenCL implementations that can be used and expanded to fill up the lack of features in the proprietary implementations until they catch up. The two most vibrant ones are Rusticl~\cite{RustiCLWeb} and Portable Computing Language (PoCL~\cite{poclIJPP}). These two projects can be utilized to extend the portability to X and Y, and interestingly, back to CUDA platforms by using the PoCL-CUDA driver.\pj{Be sure to state that some of the backends are WiP and not conformant, but can run most of the HIP tests.}

...
\pj{Discuss how we can have end-to-end testing with PoCL and support more targets thanks to its SPIR-V input.}
...
\pj{TODO: RustiCL / RadeonSI for AMD?}
...
\pj{Looping Back to NV with PoCL-CUDA and/or RustiCL}
...
\pj{TODO: A portability graph which shows the layers, required extensions, and platforms that support it. Similar to https://github.com/pocl/pocl/files/10957913/sw-stack-graph.pdf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Debugging and Profiling Support}
\label{section:debuggingAndProfiling}

Thanks to using the open standard OpenCL as the portability layer, various debugging and profiling tooling options are available to use with little to no additional effort. The following discusses a set of tools that were successfully adopted and used through $chipstar$.

\subsection{Profiling Tools}

% VTune is rather Intel device-specific, uses its monitoring
% interfaces. We cannot provide
% much on top of OpenCL directly for
% wider device support since it doesn't
% have a monitoring API. The only
% data is the profiling time stamps
% from profiling queues.
%\subsubsection{VTune}

%\pj{TODO Paulius?}

\subsubsection{Tracing Heterogeneous APIs (THAPI)}

% https://github.com/argonne-lcf/THAPI

\pj{TODO Brice?}

\subsection{Debugging}

\pj{TODO: PoCL-CPU, GDB, Valgrind, debug info...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Platform Portability Validation}
\label{section:platformPortability}

\pj{Summarize to a table with number of passing tests.}
\subsection{AMD and Intel CPUs}

\pj{Using Intel OpenCL CPU and/or PoCL-CPU}

\subsection{ARM Mali GPU}

\pj{Since ARM Mali supports SPIR-V, we could run a portability experiment with CUDA/HIP running on a smartphone....}
\pj{Does it support SVM or generic?}

\subsection{RISC-V}

\pj{If we manage to do the port in time, we could show execution on RISC-V CPUs.}

\subsection{CUDA}

\pj{Back to CUDA via PoCL-CUDA}

\subsection{PoCL-Remote}

\pj{This should be an interesting curiosity as soon as we (finally!) get PoCL-R open sourced.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Application Porting Experiments}
\label{section:applications}

In order to assess the application compatibility level of \textit{chipstar}, we ported a set of well-known applications and libraries that are commonly used with CUDA or HIP applications in HPC domain. The applications and which features or libraries of \chipstar they use are shown in Table~\ref{tab:applications}.

\newcommand{\x}[0]{\checkmark}

\pj{TODO: also highlight which compatibility library was tested}
\begin{table*}[]
    \centering
    \settowidth\rotheadsize{Warp-level primitives}
    \begin{tabular}{|c|c|c|c|c||c|c|c|c|c|c|c|}
    \hline
                                           &
                   \rothead{Shared memory} &
                   \rothead{\_\_syncthreads()} &
                   \rothead{Unified Memory} &
                   \rothead{Warp-level primitives} & \rothead{...} & \rothead{cuDNN} & \rothead{CUDA Graphs} & \rothead{hipBLAS} & \rothead{hipSOLVER} & \rothead{libCEED} & \rothead{rocPRIM} \\    
    \hline
    \hline    
       CP2K        & &  & &  &  & &  &  &  &  &  \\
    \hline
       Exabiome    & &  & &  &  & &  &  &  &  &  \\
    \hline
       GAMESS      & \x & \x &  & &  &  &  & \x & \x &  & \\ 
    \hline
       Pytorch-HIP & &  & &  &  & &  &  &  &  & \\       
    \hline
    \hline
       cuDNN       & &  & &  &  & &  &  &  &  & \\        
    \hline
       CUDA Graphs & &  & &  &  & &  &  &  &  & \\    
    \hline
       hipBLAS     & &  & &  &  & &  &  &  &  & \\
    \hline    
       hipSOLVER   & &  &  \x & & &  &  &  &  &  &  \\       
    \hline
       libCEED     & &  &  &  & & &  &  &  &  & \\
    \hline
       rocPRIM     & &  & \x & & & &  &  &  &  & \\
    \hline        
    \end{tabular}
    \caption{The applications and libraries ported on \chipstar and how each of them exercises different technically non-trivial CUDA/HIP features and \chipstar-ported libraries. \pj{Let's fill the matrix up after the table structure has stabilized.}}
    \label{tab:applications}
\end{table*}

\subsection{CP2K}

\pj{TODO Rahul?}

\subsection{Exabiome}

\pj{TODO who writes?}

\subsection{GAMESS}

\pj{TODO: Colleen? Evaluate how well it works in terms of functionality and compare to another platform in perf.} 

GAMESS (General Atomic and Molecular Electronic Structure System) \cite{gamess,gamess2} is a quantum chemistry software package which implements many electronic structure methods. 
% The code is primarily in Fortran 77/90 with some C/C++ and a CUDA library. Recently a new GPU version of the Hartree-Fock (HF) and RI-MP2 methods been implemented in CUDA which scales to 4096 nodes on Summit, an Nvidia V100-based supercomputer \cite{gamess_cuda1, gamess_cuda2, summit}. {TODO mention about how ported to HIP } Here we focus on the HF method. 

\pj{Thanks COlleen. Since we likely will lack the space in this paper we might keep the description of the GAMESS itself short and focus on the porting aspects and features it requires from CUDA/HIP. This is good.} \cb{Thanks for the comments, I updated the section taking in account your comments. Let me know if I missed something. Thanks!}
Here we focus on the Hartee-Fock (HF) algorithm used by a CUDA library in GAMESS (described extensively in Ref. \cite{gamess_cuda1}) which has been ported to HIP \cb{TODO find some reference for the porting effort}\pj{In this paper the interesting part is if we had to do modifications for the port. Stabilizing CHIP-SPV itself is not so interesting to report as it just "had to be done". From user's perspective that is not as relevant as how well it now works.}. The HF method is a common quantum chemistry method which is often the starting point for other higher-accuracy methods.  The HF method determines the molecular energy of a system by solving a set of non-linear eigenvalue equations iteratively.  It primarily involves the computation of $N^4$ two electron integrals (where $N$ is a measure of molecular system size) as well as matrix contractions of the two electron integrals once they are formed. 
% I'm ok to remove the discussion of the basis functions if the code works for all of them :) The two electron integrals are grouped into different classes, depending on the angular momentum of the basis functions used. The basis functions here are $s-$ ,$p-$, and $d-$, where $s$ is least complex and $d$ is the most complex. 
The two electron integrals are implemented as HIP/CUDA kernels which were optimized for Nvidia GPUs and total over 20,000 lines of HIP/CUDA kernel code.\pj{From this paper's perspective this sentence is very essential: The amount of code lines we successfully compile and run.} As noted in Table \ref{tab:applications}, the code uses shared memory, with \_\_syncthreads() calls to ensure copying values from global memory to shared memory completed for the threadblock before using it.
hipBLAS and hipSOLVER calls are used to form intermediates. The main hipBLAS calls are hipblasDscal, hipblasDgemm, hipblasDcopy, hipblasDaxpy, hipblasDdot, hipblasDgemv, hipblasDgeam, and the main hipSOLVER call is hipsolverDsyevd.\pj{From these it would be good to summarize for example, that they cover the key blas functionality.} These are used at each iteration of the HF algorithm to (among other things) diagonalize the Fock matrix and construct the density matrix, which are key blas functionalities in the HF algorithm.

In terms of functionality, the HF code compiles and run correctly with CHIP-SPV.
\cb{Do you think it is worth it to discuss the \_\_syncthreads issue here? My thought is that another large benefit of having this second, independent implementation of HIP allowed us to see that the HIP documentation is a little ambiguous or deficient in how it describes some things (like syncthreads)}\pj{You mean the "or exited" clause? Yes it could be useful as it's a finding while porting an app and we can refer to the extensions (Table~\ref{table:extensions})}

\cb{I need to add Performance: A100, PVC, Gen9, MI100. I should also add the peak BW and peak flops in a table}

\subsection{libCEED}

\pj{TODO: Paulius?}

\subsection{Pytorch-HIP}

\pj{TODO: Henry?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Performance Evaluation}
\label{section:performance}

In this section we evaluate the performance of \chipstar  with specific focus on overheads caused by layering HIP/CUDA on top of other API layers such as OpenCL and Level Zero.

\subsection{Execution Time vs. SYCL}

...
\pj{The specs of the GPU, its driver/oneAPI version.}
\pj{How HeCbench benchmarks were selected.}
\begin{figure*}
    \centering
    \includesvg[width=\linewidth]{figs/hecbench-intel-arc-a750}
    \caption{The execution speed of a set of benchmarks vs. SYCL on Intel ARC A750.}
    \label{fig:online-compilation}
\end{figure*}

\pj{Result analysis. Reasons for differences.}


% GeekBench 5.2.3  - 
\pj{TODO: Compare to https://github.com/vosen/ZLUDA, if possible}

\subsection{Layering Overheads}

The design of \chipstar enables different runtimes to be utilized as backends. In addition to the OpenCL backend, there is also a direct Level Zero backend which can be utilized to control Intel GPU devices. While a layered software stack, as depicted in Fig.~\ref{fig:layered-sw-stack} provides clean division of responsibilities and enables portability via a cross-vendor supported API layer, interfacing directly to vendor-specific lower level APIs such as LZ might have performance benefits.

To evaluate the costs of layering on top of OpenCL, we utilized the PoCL's experimental Level Zero driver to measure layered OpenCL calls to the Level Zero library. We utilized a set of benchmark applications \pj{todo describe} on two different Level Zero-supporting devices \pj{todo describe} and using the direct Level Zero execution as a baseline. As another comparison platform we utilized Intel's OpenCL driver.

Fig.~\ref{fig:lz-comparison} shows the comparison results. \pj{todo: to analyze after have the final numbers closer to the submission. There are inoptimalities we will fix before both in the LZ backend and in PoCL-LZ.}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering             
         \includegraphics[width=0.9\textwidth]{figs/LZ-BE-comparison-Intel-iGPU.pdf}
         \caption{iGPU}
         \label{fig:lz-comparison-igpu}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
        \includegraphics[width=1.0\textwidth]{figs/LZ-BE-comparison-Intel-dGPU.pdf}
         \caption{dGPU}
         \label{fig:lz-comparison-igpu}
     \end{subfigure}
     \caption{\chipstar direct Level Zero backend execution time (normalized to 100) in comparison to the \chipstar OpenCL backend on top of Intel OpenCL and the layered PoCL-Level0 driver with two different GPUs.}
     \label{fig:lz-comparison}
\end{figure}

\pj{TODO: Straight to CUDA vs. CUDA to HIP to chipstar to OpenCL + to OpenCL to PoCL-CUDA to CUDA}

\subsection{Graphs on Command Buffers}

\pj{Michal (?) TODO: numbers on command buffer benefits here?}

\pj{TODO: Comparisons}
\pj{TODO: Overhead analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{relatedWork}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% This doesn't fit as the page limit is only 12pp. If we resubmit to another journal, it's interesting info to add and can be easily copy-pasted from the report:

%\section{Supporting Newer CUDA Features}
%\label{section:directCUDA}

%HIP is a subset of CUDA features, roughly at version 8~\pj{check this}. Thus, it doesn't include support some of the newer features which can utilize some of the more advanced capabilities of the NVIDIA GPU platforms. Some of these features are difficult to implement efficiently on other vendors' GPU features, and since GPU offloading is primarily done with performance improvements in mind, a functional, but inefficient implementation is less interesting.

%However, for the purpose of completeness, it is interesting to highlight some of the more useful newer features in later CUDA versions, and consider implementation strategies for future work.

%\pj{Discuss features specific to CUDA, from the doc I wrote in Parmance.}


% TODO: the OpenCL extensions identified and proposed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions and Future Work}
\label{section:conclusions}

\pj{TODO: Drop-in/link-time replacement (CUDA binary compatibility)?}
\pj{TODO: Reconsider "obese" binaries: Allow adding additional target-specific binaries (but via the OpenCL binary interface) to the binary for faster initial launch.}
\pj{TODO: on-demand migrated UM (via Fine-Grained system SVM). We could implement this on CPU drivers for enabling testing such codes. Also Rusticl is adding FG SSVM support to some GPUs.}

\pj{The key pitfalls of the article currently:
\begin{itemize}
    \item The article claims cross-vendor portability but the tested devices are very Intel-centric. We should try to add at least ARM CPUs, preferably ARM GPUs. RISC-V would be excellent. Rusticl could be useful for running on AMD and NVIDIA if it works adequately well for some benchmarks (we can discuss the lack of coarse-grain SVM or subgroups, for example, if it doesn't). Running on PoCL-CUDA would also make this aspect more solid. 
    \item It reads mostly as a technical report. For a scientific article we should add a bit more complex technical content, e.g., about the Graph implementation using command buffers.
\end{itemize}}


\bibliography{IEEEabrv,chipstar}
\bibliographystyle{IEEEtran}

\end{document}
